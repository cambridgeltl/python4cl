{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "module_1.4.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "371px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4JABmLxIevG"
      },
      "source": [
        "# Python for Computational Linguists 1.4: Working with Corpus Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DytygsT5IevI"
      },
      "source": [
        "Welcome to module 1.4. In this module, we will start calculating statistics using real corpus. \n",
        "\n",
        "Let's first refresh your memory on ngrams and probabilities by completing the following quiz:\n",
        "\n",
        "## ‚ùì Pre-module Quiz\n",
        "Given the sequence `aabbdab`, what is $P(b|a)$?\n",
        "\n",
        "A. $\\large  \\frac{1}{2}$\n",
        "\n",
        "B. $\\large \\frac{1}{3}$\n",
        "\n",
        "C. $\\large \\frac{2}{3}$\n",
        "\n",
        "D. $0$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO8i5eydIevL"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>The correct answer is C.</p>\n",
        "  <p>There are in total three bigrams starting with 'a': two 'ab' and one 'aa'. $P(b|a)$ is the probability of the bigram 'ab', which is 2 out of 3, ie. 2/3. </p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyX8IrAdIevM"
      },
      "source": [
        "# Corpus Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRCIBesMIevO"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNMtNjWCIevQ"
      },
      "source": [
        "You should remember from the previous Notebook that Python organises code into units called modules. This tutorial is also based on spaCy; let's install it, then we will initialise the default English tokeniser. We will also download another library and the corpora that we need to complete this tutorial.\n",
        "\n",
        "> **Note**: if you are running this notebook on your machine, you should already have spaCy installed on your server. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cIC5EBdUQol"
      },
      "source": [
        "# download spaCy\n",
        "!pip install matplotlib\n",
        "!pip install spacy\n",
        "# download spaCy's default resources\n",
        "!python -m spacy download en_core_web_sm\n",
        "# download the Shakespeare and Marlowe corpora \n",
        "!wget https://raw.githubusercontent.com/cambridgeltl/python4cl/master/corpora/Shakespeare\n",
        "!wget https://raw.githubusercontent.com/cambridgeltl/python4cl/master/corpora/Marlowe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki5ajEJAUqgQ"
      },
      "source": [
        "**Double check**: the next output of the next cell should contain 'Marlowe' and 'Shakespeare'. If this is not the case, please contact a teaching assistant for support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AwoGAe2Upzl"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1KpZ1lhIevS"
      },
      "source": [
        "SpaCy's English class is in a script named `en.py` in the path `spacy/lang/`. To import the class variable, we could do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Z4e89BIevW"
      },
      "source": [
        "import spacy.lang.en #import the path\n",
        "nlp=spacy.lang.en.English() #call the class variable with full path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39-iNn0pIevf"
      },
      "source": [
        "import spacy.lang.en as en #this line allows Python to access code from the spacy/lang/en and to rename this code as 'en'\n",
        "nlp=en.English() #call the class variable with the name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfErFMU0Ievn"
      },
      "source": [
        "from spacy.lang.en import English # directly import the class variable\n",
        "nlp=English() # call the class directly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WolBIbkIevv"
      },
      "source": [
        "Now let's initialize a default English tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zae04sxIevw"
      },
      "source": [
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "uNXJOsbiIev3"
      },
      "source": [
        "## Processing corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdhCLsdyIev5"
      },
      "source": [
        "Python allows you to read, write and delete files. Here we'll be reading a corpus of Shakespeare's entire works (around 1 million word tokens). The relative path to the corpus file 'Shakespeare' is `./data/shakespeare` (`..` indicates the parent directory). We access this using the `open()` function. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ae58BO_Iev6"
      },
      "source": [
        "f=open('./Shakespeare','r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzZ0VmN0IewB"
      },
      "source": [
        "We declared the variable `f` to open the Shakespeare file. `open()` takes 2 arguments, the path to the file that we want to open and a string that represents the kinds of permission or operation we want to do on the file. Here the string is `'r'` which refers to the permission of 'read-only'. We can also create a new file to write by replacing `'r'` with `'w'`. If we want to append data to the file, we can replace `'r'` with `'a'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwZHSQeUIewC"
      },
      "source": [
        "Remember to close the `f` variable after finishing reading:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlym07uBIewD"
      },
      "source": [
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-ea_ETDIewJ"
      },
      "source": [
        "A more recommendable way is to use `with` keyword so that the file will be properly closed after its suite finishes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j9wl91eIewJ"
      },
      "source": [
        "with open('./Shakespeare','r') as f:\n",
        "    print ('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amGjnzXTIewS"
      },
      "source": [
        "We could use `readlines()` to read in all the lines from the corpus in to a list of strings. We store the list in the variable `lines` .   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wslBGFoeIewT"
      },
      "source": [
        "with open('./Shakespeare','r') as f:\n",
        "    lines=f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvoUaV59IewX"
      },
      "source": [
        "Let's have a look of what the variable `lines` contains by printing out the first ten items. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr5qZSzdIewZ"
      },
      "source": [
        "lines[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgBj0DEWIewf"
      },
      "source": [
        "Have you noticed that each line is followed by the special character `\\n`? This character identifies the end of a line of text, and is called a [control character](https://en.wikipedia.org/wiki/Control_character). These type of characters do not represent textual symbols but represent particular instruction (e.g. \"create a new line\") that is processed by the computer; another common control character is `\\t` or \"tabulation character\", which is inserted when you press the 'Tab' key on your keyboard. Control characters **do not** show up in text but **do show up** in string variables, so we will have to come up with a way to deal with them.\n",
        "\n",
        "Another observation is that some lines such as the last line starts with some space characters. We can get rid of these characters by calling the string method `strip()` that will automatically strip a string with whitespace characters including space, tabs, and control characters both from the start or at the end of the string. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N09wzWwWIewh"
      },
      "source": [
        "Let's write a for loop to process each line of the file and store the processed lines as tokenized word lists into the variable `lines_processed`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkLC3VKVIewi"
      },
      "source": [
        "lines_processed=[] # a list to store the processed lines\n",
        "with open('./Shakespeare','r') as f:\n",
        "    for line in f:\n",
        "        #for each line, we do:\n",
        "        #1. remove whitespace characters like \\t \\r \\n\n",
        "        line=line.strip()\n",
        "        #2. skip the empty lines\n",
        "        if line=='':\n",
        "            continue\n",
        "        else: \n",
        "            #3. tokenize the sentence into word list:\n",
        "            tokens=tokenizer(line) #the tokenizer() function that we imported return a series of token items which are now Spacy classes\n",
        "            # To convert each item in tokens to strings, we need to loop over the line again and convert each token to strings by calling str()\n",
        "            tokens_str=[]\n",
        "            for tok in tokens:\n",
        "                tokens_str.append(str(tok))# str() converts into a string\n",
        "            lines_processed.append(tokens_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7aJtH9qIewo"
      },
      "source": [
        "We can wrap up the above into a function to process files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ8bxbSTIewo"
      },
      "source": [
        "def process(filename):\n",
        "    '''\n",
        "    process file into a list of lines where each line is a list of words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : file path\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    a list of list of strings\n",
        "    [\n",
        "        [1609]\n",
        "        [THE, SONNETS]\n",
        "        ...\n",
        "    ]\n",
        "    '''\n",
        "    lines_processed=[] # a list to store the processed lines\n",
        "    with open(filename,'r') as f:\n",
        "        for line in f:\n",
        "            #for each line, we do:\n",
        "            #1. remove whitespace characters like \\t \\r \\n\n",
        "            line=line.strip()\n",
        "            #2. skip the empty lines\n",
        "            if line=='':\n",
        "                continue\n",
        "            else: \n",
        "                #3. tokenize the sentence into word list:\n",
        "                tokens=tokenizer(line) #the tokenizer() function that we imported return a series of token items which are now Spacy classes\n",
        "                # To convert each item in tokens to strings, we need to loop over the line again and convert each token to strings by calling str()\n",
        "                tokens_str=[]\n",
        "                for tok in tokens:\n",
        "                    tokens_str.append(str(tok))# str() converts into a string\n",
        "                lines_processed.append(tokens_str)\n",
        "    return lines_processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNnA3Z1tIewv"
      },
      "source": [
        "Call the function `process()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCoSIUCTIeww"
      },
      "source": [
        "lines_processed=process('./Shakespeare')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtHYP0cCIew1"
      },
      "source": [
        "Let's take a look of the lines after tokenisation. Let's print out the first ten lines:\n",
        "Do you think the tokenizer is doing well?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhLZruNQIew3"
      },
      "source": [
        "lines_processed[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgj9u_lgIew7"
      },
      "source": [
        "You can also check how many lines the corpus contains by calling `len()` function over `lines_processed`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk61wl0BIew_"
      },
      "source": [
        "len(lines_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSf8jKZVIexE"
      },
      "source": [
        "> **<h3>üíª Try it yourself!</h3>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwbjsts-IexF"
      },
      "source": [
        "In the following cell, please try processing the other corpus 'Marlowe' in the same directory of 'Shakespeare', and store the processed results in variable: `marlowe_processed`. \n",
        "\n",
        "Then, Answer the following question:\n",
        "\n",
        "How many lines does Marlowe corpus contain?\n",
        "\n",
        "A. 19492\n",
        "\n",
        "B. 114422\n",
        "\n",
        "C. 1949\n",
        "\n",
        "D. 45336"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaDrYF0sIexG"
      },
      "source": [
        "You can insert your code here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HExZE1dcIexG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkWkL1RjIexK"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>the answer is A</p>\n",
        "    <p>You can try the following code: </p>\n",
        "    <p><code>marlowe_processed=process('./Marlowe')</code></p>\n",
        "    <p><code>len(marlowe_processed)</code></p>\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "    \n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvQutg2qIexK"
      },
      "source": [
        "## Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4MhxdeoIexM"
      },
      "source": [
        "A dictionary is a collection which is unordered, changeable and indexed. \n",
        "In Python dictionaries are written with curly brackets, and they have keys and values.\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_EExjxZIexO"
      },
      "source": [
        "thisdict =\t{\n",
        "  \"brand\": \"Ford\",\n",
        "  \"model\": \"Mustang\",\n",
        "  \"year\": 1964\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XKX01VEIexS"
      },
      "source": [
        "You can access the items of a dictionary by referring to its key name, inside square brackets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSjW52bqIexS"
      },
      "source": [
        "thisdict['brand']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nsSbc2FIexX"
      },
      "source": [
        "You can change the value of a specific item by referring to its key name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKqmL_IHIexY"
      },
      "source": [
        "print(thisdict['year'])\n",
        "thisdict['year']=2019\n",
        "print(thisdict['year'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AftCgvaEIexd"
      },
      "source": [
        "Adding an item to the dictionary is done by using a new index key and assigning a value to it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV7xbX7BIexd"
      },
      "source": [
        "thisdict[\"color\"] = \"red\"\n",
        "print(thisdict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MbC-iMpIexi"
      },
      "source": [
        "You can loop through a dictionary by using a for loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wf0VZ0bIexj"
      },
      "source": [
        "#Print all key names in the dictionary, one by one:\n",
        "\n",
        "for x in thisdict:\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRTrhenvIexo"
      },
      "source": [
        "#Print all values in the dictionary, one by one:\n",
        "\n",
        "for x in thisdict:\n",
        "  print(thisdict[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmaqorTFIexs"
      },
      "source": [
        "#You can also use the values() function to return values of a dictionary:\n",
        "\n",
        "for x in thisdict.values():\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZBcuHkRIexx"
      },
      "source": [
        "#Loop through both keys and values, by using the items() function:\n",
        "for x, y in thisdict.items():\n",
        "  print(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAs2GANGIex0"
      },
      "source": [
        "To determine if a specified key is present in a dictionary use the `in` keyword:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr5mB3BSIex2"
      },
      "source": [
        "if \"model\" in thisdict:\n",
        "  print(\"Yes, 'model' is one of the keys in the thisdict dictionary\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH6ZyovLIex4"
      },
      "source": [
        "To determine how many items (key-value pairs) a dictionary has, use the `len()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8esoDZd5Iex5"
      },
      "source": [
        "#Print the number of items in the dictionary:\n",
        "print(len(thisdict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww5YIKljIex-"
      },
      "source": [
        "## Counting Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE7wVvgwIex-"
      },
      "source": [
        "Now let's loop over the word lists in `lines_processed` to create a vocabulary dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhSJi6k2Iex-"
      },
      "source": [
        "vocab={}# create an empty vocabulary dictionary to store words as keys and counts as values later. \n",
        "for line in lines_processed:\n",
        "    for word in line:\n",
        "        if word in vocab:\n",
        "            vocab[word]+=1 # update the count for an existing word\n",
        "        else:\n",
        "            vocab[word]=1 # initilize the count for a new word\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVxKPnKaIeyC"
      },
      "source": [
        "Again, we can wrap the above into a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WREb_hQpIeyD"
      },
      "source": [
        "def create_vocab_dict(f_processed_arg):\n",
        "    '''\n",
        "    Collect vocabulary counts from text\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    f_processed_arg: a list of list of words processed from text as the output of process()\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    a dictionary with words (str) as keys and counts(int) as values\n",
        "    vocab={\n",
        "    'SONNETS': 1\n",
        "    }\n",
        "    '''\n",
        "    vocab={}# create an empty vocabulary dictionary to store words as keys and counts as values later. \n",
        "    for line in f_processed_arg:\n",
        "        for word in line:\n",
        "            if word in vocab:\n",
        "                vocab[word]+=1 # update the count for an existing word\n",
        "            else:\n",
        "                vocab[word]=1 # initilize the count for a new word\n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erIyglrlIeyH"
      },
      "source": [
        "We can call the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLqz6TWCIeyI"
      },
      "source": [
        "vocab=create_vocab_dict(lines_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ENxM9fIeyM"
      },
      "source": [
        "We can then retrieve the count of a specific word by the statement `vocab[word]`, eg. the count of 'thee' in the corpus should be 3144."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ3TVM1nIeyM"
      },
      "source": [
        "vocab['thee']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zh60BYgIeyP"
      },
      "source": [
        "### ‚ùì Quiz  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuyvQc0hIeyQ"
      },
      "source": [
        "Use the following cell to retrieve the counts of the following pronouns in Shakespeare: 'thou','thee','thy','thine','you','your'. Which order of the word counts is correct?\n",
        "\n",
        "A. 'you'>'thou'>'your'>'thy'>'thee'>'thine'\n",
        "\n",
        "B. 'you'>'your'>'thou'>'thy'>'thee'>'thine'\n",
        "\n",
        "C. 'thou'>'you'>'thy'>'thine'>'your'>'thee'\n",
        "\n",
        "D. 'you'>'thou'>'your'>'thy'>'thee'>'thine'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQxhIf8WIeyQ"
      },
      "source": [
        "You can insert your code here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTKPXlEMIeyS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReTyPPQCIeyU"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>the answer is B</p>\n",
        " \n",
        "    \n",
        "\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEamJjPiIeyU"
      },
      "source": [
        "### ‚ùì Quiz  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESDGfRCJIeyV"
      },
      "source": [
        "Please collect a vocabulary dictionary from 'Marlowe' as well in the following cells, and answer the same question:\n",
        "\n",
        "Which order of the word counts is correct in Marlowe?\n",
        "\n",
        "A. 'you'>'thou'>'your'>'thy'>'thee'>'thine'\n",
        "\n",
        "B. 'you'>'your'>'thou'>'thy'>'thee'>'thine'\n",
        "\n",
        "C. 'thou'>'you'>'thy'>'thine'>'your'>'thee'\n",
        "\n",
        "D. 'you'>'thou'>'your'>'thy'>'thee'>'thine'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JHqtrldIeyW"
      },
      "source": [
        "You can insert your code here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJgvjK-FIeyW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fshWjkSWIeya"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>the answer is A</p>\n",
        "    <p>You can try the following code: </p>\n",
        "    <p><code>vocab_marlowe=create_vocab_dict(marlowe_processed)</code></p>\n",
        "        <p><code>vocab_marlowe['thine']</code></p>\n",
        "\n",
        "    \n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF_VmxMcDWjE"
      },
      "source": [
        "If you find the section easy so far, you can go on to try the following optional section on Type-token ratio. Otherwise, you can skip the following section. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYnXc8TMIeyb"
      },
      "source": [
        "## Calcuating Type-token ratio (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdJ-ZlZcIeyc"
      },
      "source": [
        "**Type-token ratio (TTR)** is the ratio obtained by dividing the **types** (the total number of different words) occurring in a text or utterance by its **tokens** (the total number of words). A high TTR indicates a high degree of lexical variation while a low TTR indicates the opposite. The range falls between a theoretical 0 (infinite repetition of a single type) and 1 (the complete non-repetition found in a concordance).\n",
        "\n",
        "Let's calculate type count first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq-UKKY_Ieyc"
      },
      "source": [
        "#Recall that vocab stores words as keys. Let's first retrieve the key list of the vocab dictioanry:\n",
        "key_list=vocab.keys()\n",
        "# the number of types is just the length of the key list\n",
        "type_count=len(vocab.keys())\n",
        "print (type_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnpxJiKmIeyg"
      },
      "source": [
        "Let's caculate token count:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz3j9HDYIeyg"
      },
      "source": [
        "# Let's create a loop to aggregate the token counts in the vocabulary:\n",
        "token_count=0\n",
        "for word in vocab:\n",
        "    token_count+=vocab[word] # equals token_count=token_count+vocab[word]\n",
        "print (token_count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qu64n8GIeyl",
        "scrolled": true
      },
      "source": [
        "#Let's calculate the type-token ratio:\n",
        "ttr=type_count/token_count\n",
        "print (ttr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v2MHk7mIey6"
      },
      "source": [
        "Alternatively, we could wrap the calculation up into a function that takes in vocab dictionary and outputs the ttr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNC_U47TIey6"
      },
      "source": [
        "def ttr_cal(vocab_arg):\n",
        "    '''\n",
        "    calculate type-token ratio\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    vocab_arg: a vocab dictionary with words as keys and counts as values\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    a float number indicating type-token ratio\n",
        "    '''\n",
        "    type_count=len(vocab_arg.keys())\n",
        "    token_count=0\n",
        "    for word in vocab_arg:\n",
        "        token_count+=vocab_arg[word]\n",
        "    ttr=type_count/token_count\n",
        "    return ttr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blNq_8uiIey-"
      },
      "source": [
        "print (ttr_cal(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kk_Fb-5IezC"
      },
      "source": [
        "> **<h3>üíª Try it yourself!</h3>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j02gUgiuIezD"
      },
      "source": [
        "Use the above functions to calculate the type-token ratio of Marlowe corpus and compare the results with the Shakespeare corpus. Who has more lexical variation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz3j9pMOIezE"
      },
      "source": [
        "You can write your code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr_9sXPrIezF"
      },
      "source": [
        "vocab_marlowe=create_vocab_dict(marlowe_processed)\n",
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx3-FZ_MIezH"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>The answer: 0.04902041788375269 (Marlowe seems to have more lexical variation)</p>\n",
        "    <p>You can try the follwing code to find out: </p>\n",
        "    <p><code>vocab_marlowe=create_vocab_dict(marlowe_processed)</code></p>\n",
        "    <p><code>ttr_cal(vocab_marlowe)</code></p>\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PElfB1IPIezI"
      },
      "source": [
        "## Plotting a frequency distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxjFuLpXIezI"
      },
      "source": [
        "To create graphs in Python, we can make use of a Python library `matplotlib`. Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. In the following line, we import the `pyplot` module from `matplotlib` and rename as `plt`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFYkG38tIezJ"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuevoThpIezL"
      },
      "source": [
        "\n",
        "Now let's try ploting a toy histogram with two words and their counts: a (3) and b (5). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6baatM4IezN"
      },
      "source": [
        "#We first create a figure:\n",
        "plt.figure(figsize=(4,4)) #4,4 determine the figure size\n",
        "#plot the historgram with the first argument as a list of labels i.e. words and the second argument as a list of counts\n",
        "plt.bar(['a','b'],[3,5]) \n",
        "#finally we show the plot by calling:\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjUhE9EWIezP"
      },
      "source": [
        "We provide the following function `produce_words_counts` to select a list of words and their counts as input for making the graph. We first sort the vocabulary from high frequency to low frequency. We can define the range of the vocabulary by changing `rank_start` and `rank_end`. `rank_start` specifies the rank of the top frequency word and `rank_end` specifies the rank of the low frequency word of the range. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiKvHgjHIezP"
      },
      "source": [
        "def produce_words_counts(vocab,rank_start,rank_end):\n",
        "    '''\n",
        "    produce words and counts for plotting graphs\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    vocab: a vocab dictionary with words as keys and counts as values\n",
        "    rank_start: the start rank of the high frequency words\n",
        "    rank_end: the end rank of the low frequency words\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    words: a list of words from rank_start to rank_end\n",
        "    counts: a list of teh respective counts of words\n",
        "    \n",
        "    '''\n",
        "    # 1. Sort the vocabulary according to frequency. It returns sorted pairs of (word,count) for the range defined between rank_start and rank_end. \n",
        "    # Please don't worry if you can't understand this part\n",
        "    vocab_sorted=sorted(vocab.items(),key=lambda x: x[1],reverse=True)[rank_start:rank_end] \n",
        "    # loop through the sorted vocabulary to get words and their respective counts for plotting the graphs later on\n",
        "    words=[]\n",
        "    counts=[]\n",
        "    for w_c in vocab_sorted:\n",
        "        w=w_c[0]\n",
        "        words.append(w)\n",
        "        count=w_c[1]\n",
        "        counts.append(count)\n",
        "    return words, counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW4jJVKgIezT"
      },
      "source": [
        "\n",
        "Try changing `rank_start` and `rank_end` to produce different `words` and `counts` lists. For example, to retrieve the top 100 words, simply run `words,counts=produce(vocab,0,100)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJddQUyyIezU"
      },
      "source": [
        "words,counts=produce_words_counts(vocab,0,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCPHgw_3IezZ"
      },
      "source": [
        "We could now plot a histogram using the words and counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmiikhXRIezZ",
        "scrolled": true
      },
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize=(15,8)) #figure size\n",
        "plt.bar(words,counts) #plot the historgram\n",
        "plt.ylim(0,90000) # specifies the minimum and maximum of the y axis. \n",
        "plt.xticks(rotation=90) # rotate the x label\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcpk6wsTIezc"
      },
      "source": [
        "> **<h3>üíª Try it yourself!</h3>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtTgNrmsIezd"
      },
      "source": [
        "Try changing `rank_start` and `rank_end` and observe what kind of words the top-frequent words are? What about the low-frequent words?  How many top-frequent words do we have? How many low-frequent words do we have?\n",
        "\n",
        "In addition, you can try changing the arguments of `plot.ylim()` to adjust the y axis. For example, you may want to lower the maximum when examining the lower rank words. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dClD-8BHIeze"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>The answer: Top frequent words tend to be function words. There are only a small number of them with very large number. There is also a long tail of low-frequent words. </p>\n",
        "    \n",
        "\n",
        " \n",
        "\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl9wstHtIeze"
      },
      "source": [
        "## Counting bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEUc5mY6Ieze"
      },
      "source": [
        "A bigram is a sequence of adjacent two words. Let's create a dictionary to store the bigrams. The key will be each bigram and the value will be counts. We can extract bigrams by looping through the items in `lines_processed`. Recall that `lines_processed` is a list that contains the tokenized corpus. \n",
        "\n",
        "We also need to insert `<start>` and `<end>` tokens in each line so that we can model probabilities of the words in the start and at the end of a sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqfaUQKlIezf"
      },
      "source": [
        "When looping through the list, we are looping over the items of the list. What if we want to loop over the index of each item as well? A useful function to use is `enumerate()` that allows us to loop over both the index and the item. We will see how it is used in the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wByx_8GEIezg"
      },
      "source": [
        "bigram_dict={}\n",
        "#update start and end token's counts in the vocab variable\n",
        "vocab['<start>']=0\n",
        "vocab['<end>']=0\n",
        "\n",
        "for line in lines_processed:\n",
        "    #insert start <start> and end <end> token, and update their unigram counts\n",
        "    line=['<start>']+line+['<end>']\n",
        "    vocab['<start>']+=1\n",
        "    vocab['<end>']+=1\n",
        "    for i,w in enumerate(line): \n",
        "        #'enumerate() loops over the variables i (the index of the current word) and w (the string of\n",
        "        #the current word)\n",
        "        w_first=w\n",
        "        if i+1<len(line): #not the end of the line\n",
        "            w_second=line[i+1]\n",
        "            bigram=(w_first,w_second) #a tuple to represent bigram\n",
        "            if bigram not in bigram_dict:\n",
        "                bigram_dict[bigram]=1\n",
        "            else:\n",
        "                bigram_dict[bigram]+=1\n",
        "                \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHZi15l8Iezh"
      },
      "source": [
        "\n",
        "> **<h3>üíª Try it yourself!</h3>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA38ao6DIezi"
      },
      "source": [
        "What is the size of the bigram dictionary ie. how many bigrams do we have in the corpus? If you compare with the size of the `vocab` which is basically a unigram dictionary, are you surprised by the difference?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W7AUmlIezi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reZE_UILIezo"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>The answer: 315400 </p>\n",
        "    <p>You can try the follwing code to find out: </p>\n",
        "    <p><code>len(bigram_dict)</code></p>\n",
        " \n",
        "\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLes7pDOIezo"
      },
      "source": [
        "> **<h3>üíª Try it yourself!</h3>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4U3qjMPIezp"
      },
      "source": [
        "Is gender associated with beauty? Try retrieving the count of 'his beauty' and 'her beauty'? Which has more counts?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIYAtg79Iezp"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>The answer: 'her beauty' occurs more often than 'his beauty', which might indicates that 'beauty' is more associated with women than with men. </p>\n",
        " \n",
        "\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR-DSk3lIezq"
      },
      "source": [
        "# Language modelling\n",
        "\n",
        "Language modelling is a fundamental task in computational lingustics where the aim is to predict the probablity of a sequence of tokens occuring. We will consider tokens to be words in this exercise, but they can be many other things depending on the application (perhaps characters, utterances, phonemes, _etc_). \n",
        "\n",
        "Formally, we want to estimate the probablity $P(w_{1},\\ldots ,w_{m})$ of an $m$-length sequence, using some given corpus.\n",
        "\n",
        "There are many techniques for language modelling; we will consider here a very simple approach called $n$-gram models. These models make the assumption that the probability of a word only depends on the previous $n-1$ context words in the sequence. If $n=1$, we call this a *unigram* model, if $n=2$, it's a *bigram* model etc.\n",
        "\n",
        "So for example, given a sequence of words: *'to be or not to'* what is the probablity the next word will be *'be'*?  In $n$-gram models, this is calculated using *conditional probablity*: $P(\\text{'be'}\\mid \\text{'to be or not to'})$ where we estimate the probablity of the entire sequence using the probablity chain rule (_i.e_. multiplying the conditional probabilities of the sequence):\n",
        "\n",
        "\n",
        "$$P(w_{1},\\ldots ,w_{m})=\\prod _{{i=1}}^{m}P(w_{i}\\mid w_{1},\\ldots ,w_{{i-1}})\\approx \\prod _{{i=1}}^{m}P(w_{i}\\mid w_{{i-(n-1)}},\\ldots ,w_{{i-1}})$$\n",
        "\n",
        "We can estimate the conditional probility from our training corpus by simply counting:\n",
        "\n",
        "$$P(w_{i}\\mid w_{{i-(n-1)}},\\ldots ,w_{{i-1}})={\\frac  {{\\mathrm  {count}}(w_{{i-(n-1)}},\\ldots ,w_{{i-1}},w_{i})}{{\\mathrm  {count}}(w_{{i-(n-1)}},\\ldots ,w_{{i-1}})}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVgjB0sy-mZ6"
      },
      "source": [
        "## Unigram language model\n",
        "Let's start by building the most basic $n$-gram model, a unigram model where the conditional probablity for each word is simply the probablity of the word occuring in the corpus data: it does not dependent on any of the previous words in the sequence.\n",
        "\n",
        "We can start by defining a function called *unigram_prob* which calculates the unigram probability given the unigram word and the vocabulary counter as input parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvoc-XHVIezr"
      },
      "source": [
        "def unigram_prob(word,vocab,token_count):\n",
        "    '''\n",
        "    produce a word's unigram probability\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    vocab: a vocab dictionary with words as keys and counts as values\n",
        "    word: a given unigram word\n",
        "    token_count: the total number of tokens in the corpus\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    prob: the unigram probability of the word\n",
        "    '''\n",
        "    prob=float(vocab[word]/token_count)\n",
        "    return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBB_veCnIezt"
      },
      "source": [
        "Let's test this by calculating the probability of the word \"horse\" occuring in the corpus of Shakespeare (remember `vocab` is the unigram dictionary we have computed for the Shakespeare corpus, and `token_count` is the total token number in the corpus):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "243T8rF9Iezt"
      },
      "source": [
        "unigram_prob(\"horse\",vocab,token_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i9W5mEIIezw"
      },
      "source": [
        "> **<h3>üíª Try it yourself!</h3>**\n",
        "\n",
        "Calculate the probability and the effect of capitalisation on the definite article. Try \"The\" vs \"THE\" vs \"the\", and note how much the probability differs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTng9zOy-mZ7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1g0zPYfIezx"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>The answer: The: 0.00354534834062168, THE: 0.00010323687303452877, the: 0.02050443124424256 </p>\n",
        " \n",
        "  <p>You can try the following code </p>\n",
        "  <p><code>unigram_prob(\"The\",vocab,token_count) </code></p>\n",
        "  <p><code>unigram_prob(\"THE\",vocab,token_count) </code></p>\n",
        "   <p><code>unigram_prob(\"the\",vocab,token_count)</code></p>\n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Ey1iOqIezx"
      },
      "source": [
        "To calculate the probability of a sequence (a sentence, paragraph, _etc_.) occurring, we use the chain rule of probability by multiplying the unigram probability of individual words. So lets calculate the probability of the sequence: *'To be, or not to be, that is the question:'*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi573_1cIezx"
      },
      "source": [
        "unigram_prob(\"To\",vocab,token_count) * unigram_prob(\"be\",vocab,token_count) * unigram_prob(\"or\",vocab,token_count)* unigram_prob(\"not\",vocab,token_count) * unigram_prob(\"to\",vocab,token_count) * unigram_prob(\"be\",vocab,token_count) * unigram_prob(\",\",vocab,token_count) * unigram_prob(\"that\",vocab,token_count) * unigram_prob(\"is\",vocab,token_count) * unigram_prob(\"the\",vocab,token_count) * unigram_prob(\"the\",vocab,token_count) * unigram_prob(\"question\",vocab,token_count) * unigram_prob(\":\",vocab,token_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Um9q4jAIez0"
      },
      "source": [
        "The probablity value of our example sequence is a very small number! In fact, it is so small that it's about the same probability of picking the same ant thrice at random from all the ants on the planet!\n",
        "\n",
        "What happens if we continue to add further words to this sequence?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCeX8_m6Iez0"
      },
      "source": [
        "## Log-probability\n",
        "\n",
        "Eventually the probability values will get so tiny that computers will not be able to represent them correctly in memory; this is known as an *underflow error*.  It will not require much more text to get an underflow error: a simple paragraph using our unigram model will result in zero! \n",
        "\n",
        "Fortunately, there is a very simple way to avoid this: we can use log-probability instead of regular probability. To do so, we can use python's pre-built function `log()` from the `math` package. We can simply use the log function in our unigram_prob function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Kspnf5Iez1"
      },
      "source": [
        "import math\n",
        "\n",
        "def unigram_prob(word,vocab,token_count):\n",
        "    '''\n",
        "    produce a word's unigram log probability\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    word: a given unigram word\n",
        "    vocab: a vocab dictionary with words as keys and counts as values\n",
        "    token_count: the total count of all words\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    prob: the unigram log probability of the word\n",
        "    '''\n",
        "    logprob=math.log(float(vocab[word]/token_count))\n",
        "    return logprob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VYz_WXfIez4"
      },
      "source": [
        "Now let's try calculacting the log-probability for the word \"horse\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz1AUnS-Iez4"
      },
      "source": [
        "unigram_prob(\"horse\",vocab,token_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu9W6WTZIez7"
      },
      "source": [
        "To calculate the log-probability of an entire sequence, we simply add the individual log-probablity values of individual unigrams instead of multiplying them. \n",
        "\n",
        "Let's write a function that calculates the log-probability of a given text sequence (as a single string). The function must tokenize the input string correctly before performing the calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocO-MSXDIez7"
      },
      "source": [
        "def unigram_prob_for_sequence(tokens,vocab,token_count):\n",
        "    '''\n",
        "    produce the total unigram log probability of a sequence\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    tokens: a list of words in the sequence\n",
        "    vocab: a vocab dictionary with words as keys and counts as values\n",
        "    token_count: the total count of all words\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    prob_sum: the sum of the unigram log probability of a sequence\n",
        "    '''\n",
        "    unigram_probs=[]\n",
        "    for tok in tokens:\n",
        "        unigram_probs.append(unigram_prob(tok,vocab,token_count))\n",
        "    prob_sum=sum(unigram_probs)\n",
        "    return prob_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Biev9MIez-"
      },
      "source": [
        "Let's now calculate the log-probablity of our example sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAKUST65Iez_"
      },
      "source": [
        "text = \"To be, or not to be, that is the question:\"\n",
        "example_sequence = [str(tok) for tok in tokenizer(text)]\n",
        "unigram_prob_for_sequence(example_sequence,vocab,token_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8SRJEt4Ie0C"
      },
      "source": [
        "> **<h3>üíª Try it yourself!</h3>**\n",
        "\n",
        "Calculate the log-probability of the sentence: *'A horse, a horse! My kingdom for a horse!'*. \n",
        "\n",
        "Is this sequence more or less likely to occur than *\"To be, or not to be, that is the question:\"* ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akVIYHeo-mZ8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcHvzU9oIe0H"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>The answer: -74.2155017817561 (much less likely to occur than \"To be, or not to be, that is the question:\"</p>\n",
        "    <p>You can try the follwing code to find out: </p>\n",
        "    <p><code>text = \"A horse, a horse! My kingdom for a horse!\"</code></p>\n",
        "    <p><code>tokens = [str(tok) for tok in tokenizer(text)]</code></p>\n",
        "    <p><code>unigram_prob_for_sequence(tokens,vocab,token_count)</code></p>\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25zZ1wSdIe0L"
      },
      "source": [
        "## Bigram language model\n",
        "\n",
        "Unlike the unigram language model above, a bigram language model uses the context history. It uses conditional probability to estimate the probability of a word occuring relative to the occurance of its predecessor in the sequence:\n",
        "\n",
        "$$P(w_{i}\\mid w_{i-1})={\\frac  {{\\mathrm  {count}}(w_{i-1},w_{i})}{{\\mathrm  {count}}(w_{i-1})}}$$\n",
        "\n",
        "Let's implement a bigram_prob function that calculates the bigram conditional probability for a single token:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ6ZTOD3Ie0M"
      },
      "source": [
        "def bigram_prob(w1, w2,vocab,bigram_dict):\n",
        "    '''\n",
        "    produce the bigram conditional log probability of P(w2|w1)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    w1: the first word of the bigram\n",
        "    w2: the second word of the bigram\n",
        "    vocab: a vocab dictionary with words as keys and counts as values\n",
        "    bigram_dict: a bigram dictionary with bigrams (tuple of words) as keys and counts as values\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    logprob: the sum of the unigram log probability of a sequence\n",
        "    '''\n",
        "    logprob=math.log(float(bigram_dict[(w1,w2)]/vocab[w1]))\n",
        "    return logprob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8s1r-PgIe0O"
      },
      "source": [
        "Let's test this by calculating the conditional log-probability for the bigram \"to be\", i.e., the log-probability of the word \"be\" given the word \"to\" appearing before it. We will use the unigram dictionary `vocab` and the bigram dictionary `bigram_dict` computed from Shakespeare. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKOgjnPOIe0P"
      },
      "source": [
        "bigram_prob(\"to\", \"be\",vocab,bigram_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRSJP2WBIe0R"
      },
      "source": [
        "As before, we can write a function to calculate the log-probability for a sequence of text using our bigram language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSEf2InXIe0S"
      },
      "source": [
        "def bigram_prob_for_sequence(tokens,vocab,bigram_dict):\n",
        "    '''\n",
        "    produce the total unigram conditional log probability of a sequence\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    tokens: a list of words in the sequence\n",
        "    vocab: a vocab dictionary with words as keys and counts as values\n",
        "    bigram_dict: a bigram dictionary with bigrams (tuple of words) as keys and counts as values\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bigram_prob_sum: the sum of the bigram log probability of a sequence\n",
        "    '''\n",
        "    bigram_probs=[]\n",
        "    token_len=len(tokens)\n",
        "    tokens=['<start>']+tokens+['<end>']\n",
        "    for i,w in enumerate(tokens): \n",
        "        #'enumerate() loops over the variables: i (the index of the current word) and w (the string of\n",
        "        #the current word)\n",
        "        w_first=w\n",
        "        w_second=tokens[i+1]\n",
        "        if w_second=='<end>': # until the end token of the line\n",
        "            break\n",
        "        bigram_probs.append(bigram_prob(w_first,w_second,vocab,bigram_dict))\n",
        "    assert token_len==len(bigram_probs)\n",
        "    bigram_prob_sum=sum(bigram_probs)\n",
        "    return bigram_prob_sum\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkRAKuwAIe0V"
      },
      "source": [
        "Let's use this function to calculate the log probability of our example sequence, and compare it to the unigram language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g355VwePIe0V"
      },
      "source": [
        "unigram_prob_for_sequence(example_sequence,vocab,token_count) # log-probability using unigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbWhr4qLIe0c"
      },
      "source": [
        "bigram_prob_for_sequence(example_sequence,vocab,bigram_dict) # log-probability using bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFfMaU_pIe0e"
      },
      "source": [
        "It is interesting to note that the log-probability value using the bigram model is substantially larger (the sequence is much likelier to occur). Is this perhaps because the bigram language model takes the context history into account whereas a unigram language model does not?\n",
        "\n",
        "We discuss how to correctly evaluate language models in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfWDpB4vIe0f"
      },
      "source": [
        "> **<h3>üíª Try it yourself!</h3>**\n",
        "\n",
        "Calculate and compare the unigram and bigram log-probabilities of the sentence: *'Double, double, toil and trouble; fire burns, and cauldron bubble.'*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uTWASmo-mZ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv8kRHa_Ie0h"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the code and the answer.</summary>\n",
        "    <p>The answer: Unigram: -107.52222338091987, Bigram: -76.82963768868697</p>\n",
        "    <p>You can try the follwing code to find out: </p>\n",
        "    <p><code>text = \"Double, double, toil and trouble; fire burns, and cauldron bubble.\"</code></p>\n",
        "    <p><code>tokens = [str(tok) for tok in tokenizer(text)]</code></p>\n",
        "    <p><code>unigram_prob_for_sequence(tokens,vocab,token_count) # log-probability using unigram</code></p>\n",
        "    <p><code>bigram_prob_for_sequence(tokens,vocab,bigram_dict) # log-probability using bigram </code></p>\n",
        "\n",
        " \n",
        "\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZySPmxTDIe0h"
      },
      "source": [
        "## Evaluating language models\n",
        "\n",
        "Language models are evaluated using *perplexity* ($P$), a measure of how well a probability model predicts a given sample (e.g. text sequences).\n",
        "\n",
        "Perplexity is defined as $P = 2^{{h(s)}}$ where $h$ is the information entropy per word for a given input $s$. $h(s)$ can be calculated as follows:\n",
        "\n",
        "$$h(s)=\\frac{-1}{|s|}\\sum _{w \\in s}\\log _{2}P(w)$$\n",
        "\n",
        "Here, $s$ is some sample text and $|s|$ is the number of tokens in $s$.\n",
        "\n",
        "The value of perplexity can be thought of as the number of choices the model needs to make on average per token of the input sequence. The lower the perplexity score, the better the language model is since the model has fewer choices on average for the input sequence. \n",
        "\n",
        "Let's implement a simple function to calculate the perplexity, taking as input the sum of the log-probability for a sequence (calculated by a language model) and the length of a sequence (in number of tokens)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcOtTBukIe0i"
      },
      "source": [
        "def perplexity(log_prob, length):\n",
        "    '''\n",
        "    produce the per token perplexity of a log probability of a sequence\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    log_prob: the sum of the log-probability of a sequence\n",
        "    length: length of the sequence\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    p: token-level perplexity\n",
        "    '''\n",
        "    H =  - (log_prob/ math.log(2)) / length  #change of log-base from base 10 to base 2, then normalize by length\n",
        "    p = math.pow(2,H)\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Y6rvt5Ie0k"
      },
      "source": [
        "Let's calculate the perplexity of both language models by simply calling the respective functions, and feed the results to the perplexity function above along with the length of the example sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVxUA3BPIe0k"
      },
      "source": [
        "perplexity(bigram_prob_for_sequence(example_sequence,vocab,bigram_dict),len(example_sequence)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHWG6y8VIe0n"
      },
      "source": [
        "perplexity(unigram_prob_for_sequence(example_sequence,vocab,token_count),len(example_sequence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr5X0I-V-mZ9"
      },
      "source": [
        "# ‚úçÔ∏è Assessment\n",
        "The following exercises should be done individually and shown to your assessor in Week 7 of term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BACWmfy5Ie0q"
      },
      "source": [
        "1. [1 mark] Please calculate the per token unigram and bigram perplexity for the entire Shakespeare corpus. Which is lower?\n",
        "(Hint: you can split the corpus into sentences, and calculate per token perplexity for each sentence. You can then take an average of all the sentences' per token perplexity in a corpus as the perplexity of the corpus)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ZIN7DTeEL-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaalquwcfCQs"
      },
      "source": [
        "\n",
        "2. [2 marks] Can you use the pre-defined functions so far to calculate the per token unigram and bigram perplexity of the Marlowe corpus? Compare with the Shakespeare corpus, which corpus has lower perplexity? What does it tell you?\n",
        "(Hint: You need to produce the unigram dictionary, the total token count and bigram dictionary from the Marlowe corpus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWF7ogMtfDbL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZ4wmODIe0q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Wcxkvz-mZ9"
      },
      "source": [
        "# Survey\n",
        "\n",
        "Please complete the [post-module survey](https://docs.google.com/forms/d/e/1FAIpQLSeLX1N344kBn8q9PTgg455lrzvVzzI5IW9itF4cT_WqeQKaFQ/viewform) when you are finished. Thank you!"
      ]
    }
  ]
}