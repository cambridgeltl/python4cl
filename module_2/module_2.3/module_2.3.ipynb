{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "module_2.3.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlGf-HQ3DO2A",
        "colab_type": "text"
      },
      "source": [
        "<div style='background-image: url(\"../../resources/section_header.jpg\") ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
        "    <div style=\"float: right ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.7) ; width: 50% ; height: 150px\">\n",
        "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
        "            <div style=\"font-size: x-large ; font-weight: 900 ; color: rgba(0 , 115 , 207 , 0.9) ; line-height: 100%\">Python for Computational Linguists</div>\n",
        "            <div style=\"font-size: x-large ; padding-top: 20px ; color: rgba(0 , 115 , 207, 0.7)\">2.3 Part-of-speech Tagging</div>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJewFf6DDO2C",
        "colab_type": "text"
      },
      "source": [
        "# Module 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHXX_pVRDO2F",
        "colab_type": "text"
      },
      "source": [
        "We agreed to be flexible on the content and just to include as much as could comfortably fit into 2 hours with the potential for extensional activities.\n",
        "ïƒŸ material from http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf\n",
        "ïƒŸ data from\n",
        "http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html\n",
        "\n",
        "* Summary of key objectives\n",
        "** understand that many language processing tasks, such as part of speech tagging, can be viewed as text classification;\n",
        "** understand that part of speech tagging is an important early application of sequence classification;\n",
        "** see in practice how a simple n-gram model can be used to perform part of speech tagging;\n",
        "** see in practice how linguistic features found in corpora can help language models to understand linguistic patterns, and be used to make predictions about new language data;\n",
        "\n",
        "* Quick introduction (with material taken from the NLTK book and the lecture notes):\n",
        "** What is part of speech tagging?\n",
        "** What is a tagged corpus?\n",
        "** Using spaCyâ€™s part of speech tagger to process a sequence of words\n",
        "Quick reminder of how to use spaCyâ€™s POS tagger and some of the issues we raised, e.g. about domain adaptation, data quantity and quality. E.g. get the students to POS tag two contrasting sentences, e.g. lexically ambiguous homophones - one with a common homonym sense and another with a rare homonym sense (e.g. â€˜It is wrong to object to this objectâ€™ or â€˜I must present the present on his birthdayâ€™, â€˜The insurance for the invalid was invalidâ€™), or one sentence with a neologism and one without a neologism or one with a nonsense word/unknown word that is not a common noun (e.g. â€˜he was scrobblingâ€™). Encourage the students to play with the tagger by submitting their own challenging sentences.  Get the students to notice the set of POS tags and their meanings.  Get the students to see what happens if you normalise the text to lower case. \n",
        "In Module 1 we saw how to call spaCyâ€™s POS tagger using a function call.  Whilst spaCyâ€™s POS tagger is very good, weâ€™re going to be going hands on by building our own POS simple n-gram tagger using the NLTK library.\n",
        "* Todayâ€™s goal: building and testing a part of speech tagger with NLTK\n",
        "* Getting started\n",
        "** Exploring tagged corpora (section 5.2 in the NLTK book)\n",
        "Look at several POS tagged corpora, e.g. the Brown corpus and the GENIA corpus (see data at http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html). \n",
        "Try to get the students thinking about the POS tag set â€“ what do the categories mean?  Get the students to explore the corpora and gain insights, e.g. most frequent POS tag, least frequent POS tag, plotting the curve of the POS tag frequency distribution.  What kinds of words occur in the noun, adjective, adverb category?  \n",
        "Look at some n-gram sequences of POS tags to get the students thinking about n-grams.\n",
        "Look at some words which are POS ambiguous.\n",
        "** Mapping words to properties using Python dictionaries (section 5.3 in NLTK)\n",
        "Get the students thinking about linguistic objects as data structures, e.g. a dictionary could map from a headword to a POS or to a specific sense, e.g. in WordNet or Wikipedia. \n",
        "Dictionaries in Python again. Defining a POS dictionary by hand.  Creating a POS dictionary using a tagged corpus. \n",
        "** Creating a bigram dictionary\n",
        "Page 196 in the NLTK book shows how to create a dictionary of words and tags to perform bigram tagging;\n",
        "If we try to create an n-gram tagger for larger values of n what will happen?  Weâ€™ll encounter the sparse data problem and also start to reach the limits of computer memory.\n",
        "* Automatic tagging (section 5.4 in the NLTK book)\n",
        "** Separating training and testing data\n",
        "From page 225 in the NLTK book â€“ uses the Brown corpus to make a 90:10 split\n",
        "Get the students to fill in a LaTeX table that characterises the data: distribution of tags in the whole corpus, in the 90% training set and the 10% testing set.\n",
        "** A simple default tagger \n",
        "Assigns the NN to each word in the Brown training corpus\n",
        "Evaluate it using the Brown testing corpus with recall, precision and F1 for each category\n",
        "Get the students to measure the out of vocabulary rate between the training and test data sets\n",
        "-\tAsk the students to report the number of types and tokens in the 90% training set, the 10% testing set and the OOV rate.\n",
        "Get the students to look and learn from the evaluation data\n",
        "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
        "** A regular expression tagger ïƒŸ cut out\n",
        "Again, evaluate it using recall, precision and F1 for each category \n",
        "Again encourage the students to look and learn from the evaluation data\n",
        "Get the students to construct new regular expression patterns and to evaluate them\n",
        "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "** The most frequent 100 unigram tagger\n",
        "Again, evaluate it using recall, precision and F1 for each category \n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "Again encourage the students to look and learn from the evaluation data\n",
        "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "** A simple bi-gram tagger\n",
        "Emphasize that a real n-gram tagger would do decoding using a dynamic programming algorithm called Viterbi (the students will have heard of this in the lecture).  Today in the interests of time we are not going to do decoding, weâ€™re just going to assign the most frequent POS sequence to each bigram that matches (if indeed a bigram does match).\n",
        "Again, evaluate it using recall, precision and F1 for each category \n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "Low accuracy because of data sparsity â€“ most bigrams were never seen during training.\n",
        "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "** Combining a models with backoff (e.g. unigram plus bigram tagger)\n",
        "** Further insights from evaluation using a confusion matrix\n",
        "* Reminder of key objectives\n",
        "** understand that many language processing tasks, such as part of speech tagging, can be viewed as text classification;\n",
        "** understand what a part of speech tag set is;\n",
        "** see in practice how a simple n-gram model can be used to perform part of speech tagging;\n",
        "** see in practice how linguistic features found in corpora can help language models to understand linguistic patterns, and be used to make predictions about new language data;\n",
        "\n",
        "* Homework: \n",
        "** (1 point): Run your bigram tagger on this made-up sentence â€˜@Will WOOOHOO Will the New jPhone bOut 2night? Soexcited :)â€˜. Does the tagger get the correct answer?  Write down two challenges the tagger has incorrectly resolved. ïƒŸ looking here for an OOV word (WOOOHOOO) and  failure to disambiguate homonyms (e.g. Will).  The tagger may also not handle slang (2night), neologisms (jPhone) and tokenization errors (Soexcited).\n",
        "** (2 points): Write a program to find out:\n",
        "a.\twhich word has the most POS tags in the GENIA corpus.\n",
        "b.\twhat percentage of words in the GENIA corpus are ambiguous, i.e. have more than one POS tag.\n",
        "** (2 points):  Write a program to replace low frequency words (words with a frequency of 5 or less) with UNK in the Brown corpus.  How much does this improve your backoff taggerâ€™s F-score performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufigb6jFDO2H",
        "colab_type": "text"
      },
      "source": [
        "This module is built based on the lecture note about part-of-speech tagging and Chapter 5 of the [book](http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf), and students are strongly recommended to go through them.\n",
        "\n",
        "In this module, we are going to\n",
        "- a\n",
        "- b\n",
        "- c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k9F2MOdDO2J",
        "colab_type": "text"
      },
      "source": [
        "# Python for Computational Linguists 1.4: Part-of-speech Tagging (POS Tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GgJipxvDO2L",
        "colab_type": "text"
      },
      "source": [
        "### What is POS tag?\n",
        "For each word in context, we can assign a **lexical category**, such as noun, verbs, adjectives, etc.\n",
        "These categories are often referred to as a word's part-of-speech tag or POS tag.\n",
        "The set of all POS tags is called a **tagset**, and the activity of assigning these tags to words is referred to as **part-of-speech tagging** or simply as **POS tagging**.\n",
        "\n",
        "The number of categories we require for our tagset often depends on both linguistic and practical considerations.\n",
        "Very commonly used tagsets include the 87-tag Brown set, 45-tag Penn Treebank set, the 61-tag CLAWS 5 (C5), and the 17-tag Universal POS tagset. \n",
        "[Here](https://universaldependencies.org/u/pos/) is the list of tags of Universal POS tagset.\n",
        "Please check lecture note for more detials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5rNkKVYDO2M",
        "colab_type": "text"
      },
      "source": [
        "### Why do we want to do POS tagging?\n",
        "As the POS tag of a word will give a large amount of information about this word and its neighbours, POS tagging can help understanding better texts, beneficial to many dowstream NLP tasks such as:\n",
        "\n",
        "1. Disambiguating word senses. Consider the following example for the heterophone *content*:\n",
        "    1. There was very little *content* to the essay.\n",
        "    2. The sleepy pug puppy was very *content*.\n",
        "  \n",
        "  or homonyms:\n",
        "  - It is wrong to *object* to this *object*.\n",
        "  - I must *present* the *present* on his birthday.\n",
        "  - The insurance for the *invalid* was *invalid*.\n",
        "\n",
        "2. Mitigating the issue of data sparsity. \n",
        "    - Label named entities like people, places or organizations as part of information extraction systems. In the following example, both *Chase Manhattan* and *J.P. Morgan* should be detected as proper nouns:\n",
        "        - Chase Manhattan and its merger partner J.P. Morgan.\n",
        "    \n",
        "    - Deal with out-of-vocabulary (OOV) words such as neologism words or acronyms in the social media: \n",
        "        - @username its #awesome u gonna â™¥ it Chk out our cooool project on some_url + RT it.\n",
        "\n",
        "\n",
        "More generally, being able to automatically perform POS tagging will help reduce the laborious human effort required to parse a sentence, and it will be the main goal of this module, which is to build such an automatic model/tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czrMx4pUDO2S",
        "colab_type": "text"
      },
      "source": [
        "## â“ Pre-module quiz\n",
        "\n",
        "TODO: any quiz more interactive????"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIuXAYe_DO2U",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1De39wDDO2W",
        "colab_type": "text"
      },
      "source": [
        "Before building our own tagger, we are going to use the NLTK library to load some existing tagger to perform POS tagging, and some tagged corpora will be introduced for further training and evaluations of our models.\n",
        "First let's import the [NLTK](https://www.nltk.org/) library and download some necessary resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6S2SodjDO2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.__version__\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('treebank')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6rVLyc-DO2i",
        "colab_type": "text"
      },
      "source": [
        "Given the input text, we first tokenize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZLMREilDO2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'I like this module and NLTK library very very much!'\n",
        "tokenized_text = nltk.word_tokenize(text)\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMsX-kdtDO2q",
        "colab_type": "text"
      },
      "source": [
        "We can see that the tokenized text is a list where each element is a token. \n",
        "\n",
        "Now let's tag the tokenize text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ynlt2jeDO2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagged_text = nltk.pos_tag(tokenized_text)\n",
        "print(tagged_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYmkVSXXDO2z",
        "colab_type": "text"
      },
      "source": [
        "The returned tagged text is a list, where each element is a tuple. For each tuple, the first element is the previous token, and the second element is its corresponding tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3tWv279DO20",
        "colab_type": "text"
      },
      "source": [
        "NLTK provides documentation for each tag, which can be queried using the tag, e.g., `nltk.help.upenn_tagset('PRP')`, or a regular expression, e.g., `nltk.help.upenn_brown_tagset('NN.*')`. Some corpora have README files with tagset documentation; see `nltk.name.readme()`, substituting in the name of the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "1_cqoTAsDO21",
        "colab_type": "text"
      },
      "source": [
        "> **<h3>ðŸ’» Try it yourself!</h3>**\n",
        "\n",
        "Use the current tagger to tag the previous text examples to see if the key words (heterophone, homonym, named entity and neologism) are correctly tagged.\n",
        "What are possible explanations to the wrong words?\n",
        "What happens if we normalise the text to lower case?\n",
        "Play with the tagger by submitting your own challenging sentences.\n",
        "\n",
        "- There was very little *content* to the essay.\n",
        "- The sleepy pug puppy was very *content*.\n",
        "- It is wrong to *object* to this *object*.\n",
        "- I must *present* the *present* on his birthday.\n",
        "- The insurance for the *invalid* was *invalid*.\n",
        "- They *refuse* to permit us to obtain the *refuse* permit\n",
        "- Chase Manhattan and its merger partner J.P. Morgan.\n",
        "- @username its #awesome u gonna â™¥ it Chk out our cooool project on some_url + RT it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgDVZ4Q3DO24",
        "colab_type": "text"
      },
      "source": [
        "TODO: have answers ready and ask students to identify any wrong ones, and ask them why? \n",
        "change the sentences to let the tagger change the decision wrong -> correct, correct -> wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzs8c8g1DO25",
        "colab_type": "text"
      },
      "source": [
        "### Connection with Text Classification\n",
        "\n",
        "As we have seen in previous modules, text classification is the task where we assign a label to the whole sequence of text, e.g. document. POS tagging can be also viewd similarly. Instead of assigning a label on the document level, we assign a label to each word or token. This type of task is usually called **sequence labeling** or **sequence classification**, and POS tagging is one of the most important tasks of sequence classification in NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBKhxCS4DO26",
        "colab_type": "text"
      },
      "source": [
        "### Tagged corpora\n",
        "POS taggers using machine learning techniques generally require annotated data to train on.\n",
        "For all tagging models, including those that do not require any training data like regex- or rule-based models, an annotated evaluation dataset is also required to measure the performance of these models by comparing the model outputs with the gold standard annotations.\n",
        "\n",
        "Tagged corpora, generally tagged by an human expert, have the form similar to the following:\n",
        "\n",
        "```\n",
        "The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
        "```\n",
        "Different datasets might have different forms, but each word (token) in sentences will be generally accompanied by a POS tag to form the pair **word/tag**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reStt-rwDO27",
        "colab_type": "text"
      },
      "source": [
        "Let's explore the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus) and the [Penn Treebank Corpus](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf) annotated with POS tags.\n",
        "Both corpora are already included in NLTK with POS tags, and we can get the full text by calling `tagged_words()` or `tagged_sents()` when the corpus is also segmented into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUODO8NMDO28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read Brown Corpus\n",
        "print(nltk.corpus.brown.tagged_words())\n",
        "print(nltk.corpus.brown.tagged_sents())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkbiwdJvDO3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read Penn Treebank Corpus\n",
        "print(nltk.corpus.treebank.tagged_words())\n",
        "print(nltk.corpus.treebank.tagged_sents())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOwsxj-5DO3N",
        "colab_type": "text"
      },
      "source": [
        "Did you notice that the tags used in the two corpora seem different? They are indeed using different tagsets, and to investigate what each tag category means, we can check both tagsets: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-9yjw0ADO3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Brown tagset\n",
        "nltk.help.brown_tagset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt9h5VacDO3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Penn Treebank tagset\n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj9JK_okDO3j",
        "colab_type": "text"
      },
      "source": [
        "Taking the Brown Corpus as an example, we can explore the corpus further by counting how many times a tag appears in the corpus, and create a distribution of tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_eA3zroDO3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_fd = nltk.FreqDist(tag for (word, tag) in nltk.corpus.brown.tagged_words())\n",
        "tag_fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hl5Pt4EDO32",
        "colab_type": "text"
      },
      "source": [
        "We can find the most and least frequent POS tag by sorting the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay1TuyjsDO34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_list = sorted(tag_fd.items(), key=lambda kv: kv[1], reverse=True)\n",
        "print(tag_list[0])\n",
        "print(tag_list[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCf4So3UDO4B",
        "colab_type": "text"
      },
      "source": [
        "We can also plot the curve of the POS tag frequency distribution. For the simplicity of visualization, let's first map each tag name to an index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ytBRfCHDO4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_fd_index = dict(zip([tag for tag, freq in tag_list], range(len(tag_list))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQPgkz4oDO4M",
        "colab_type": "text"
      },
      "source": [
        "Then let's create a new `FreqDict` of tag_index and its frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER4TibGNDO4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_index_fd = nltk.FreqDist({tag_fd_index[k]: v for k, v in tag_fd.items()})\n",
        "tag_index_fd.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arHQRd8hDO4W",
        "colab_type": "text"
      },
      "source": [
        "Finally, letâ€™s look for words that are highly ambiguous as to their POS tag. Understanding why such words are tagged as they are in each context can help us clarify the distinctions between the tags.\n",
        "\n",
        "Note that the items being counted in the frequency distribution are word-tag pairs, we can then treat the word as a condition and the tag as an event, and initialize a conditional frequency distribution with a list of condition-event\n",
        "pairs. This lets us see a frequency-ordered list of tags given a word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X81wWfpLDO4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = nltk.ConditionalFreqDist((word.lower(), tag) for (word, tag) in nltk.corpus.brown.tagged_words())\n",
        "print(data.conditions()[0])\n",
        "data['the']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQfIr67jDO4e",
        "colab_type": "text"
      },
      "source": [
        "We can print first 20 words along with their POS tags when the word has more than 4 tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGgJZspkDO4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 0\n",
        "for word in data.conditions():\n",
        "    if len(data[word]) > 4:\n",
        "        tags = list(data[word].keys())\n",
        "        print (word, ' '.join(tags))\n",
        "        idx += 1\n",
        "        if idx >= 20:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCcVirkrDO4k",
        "colab_type": "text"
      },
      "source": [
        "> **<h3>ðŸ’» Try it yourself!</h3>**\n",
        "\n",
        "For Penn Treebank Corpus, repeat the same process, i.e. plotting the distribution of the tagset, and finding the most and least frequent tag.\n",
        "\n",
        "For both corpora, what kinds of words occur in the noun, adjective, adverb category?\n",
        "\n",
        "Explore the POS ditribution of ambiguous words mentioned before, e.g. *content* and *present*. Do they support our observations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHuFNJ_oDO4n",
        "colab_type": "text"
      },
      "source": [
        "#### POS tag n-grams\n",
        "So far we have been working with a only single POS tag in text sequence, i.e. the unigram. \n",
        "However, we should be aware that POS each tag in text sequence often depends on its previous/later tags.\n",
        "For example, nouns can appear after determiners and adjectives, and can be the subject or object of the verb.\n",
        "Therefore, we can look at some n-gram sequences of POS tags such as tag bi-grams.\n",
        "\n",
        "Let's build a frequency dictionary of tag bi-grams for the Brown Corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J-rbYO-DO4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_bigram_list = list(nltk.bigrams(nltk.corpus.brown.tagged_words()))\n",
        "tag_bigram_fd = nltk.FreqDist((a[1], b[1]) for (a, b) in tag_bigram_list if b[1])\n",
        "tag_bigram_fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piNIlFnxDO4u",
        "colab_type": "text"
      },
      "source": [
        "We can see that compared to unigram tag frequency dictionary, bigram dictionary has many more entries. It can potentiall cause the data sparsity problmen as we will encounter later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKICrzikDO4w",
        "colab_type": "text"
      },
      "source": [
        "To validate our intuitions that nouns often occur after the determiners and adjectives, we can investigate the distribution of tag bi-gram **(X, NN)** where **X** refers to any possible tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMQa50ZoDO4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_bigram_fd = nltk.FreqDist({k: v for k, v in tag_bigram_fd.items() if k[1] == 'NN'})\n",
        "sorted_nn_bigram_list = sorted(nn_bigram_fd.items(), key=lambda kv: kv[1], reverse=True)\n",
        "print(sorted_nn_bigram_list[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ49CtaaDO45",
        "colab_type": "text"
      },
      "source": [
        "We can see that the two most frequent tags with **(X, NN)** are indeed **AT** (article) and **JJ** (adjective) according to the Brown Corpus tagset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A93RhIh2DO47",
        "colab_type": "text"
      },
      "source": [
        "### Mapping of properties using Python dictionaries\n",
        "As we have seen, a tagged word of the form **(word, tag)** is an association between a word and a POS tag. \n",
        "Once we start doing POS tagging, we will be using such structures extensively.\n",
        "For example, we want to get the tag list of a given word **(word, tag list)**, or the frequency of a given tag **(tag, freq)**.\n",
        "We can think of this process as mapping from a certain property to another, and the most natural way to store mappings in Python uses the so-called `dictionary` data type, which we have encountered in module 1 and previous part of this module. \n",
        "\n",
        "We look at dictionaries and see how they can represent a variety of language information. \n",
        "For example, recall the frequency dictionary we used previously, which maps from a POS tag to its frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_QqJciyDO48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tag_fd['NN'])\n",
        "tag_fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjblRyhkDO5P",
        "colab_type": "text"
      },
      "source": [
        "If we want to create a dictionary of the form **(word, its corresponding tags)**, what types would be for the key and value of the dictionary?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPwzLATqDO5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2tags = {'present': 'V', 'object': 'N'}\n",
        "for k, v in word2tags.items():\n",
        "    print('{}: {}'.format(k, word2tags[k]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEZTe3sZDO5W",
        "colab_type": "text"
      },
      "source": [
        "Here we use the `string` type for tag, which can be problematic, as both words *present* and *object* can be either noun and verb, so if we want to change a tag of *present*, the original tag will be overwritten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3OuZwO9DO5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = 'present'\n",
        "print('{}: {}'.format(word, word2tags[word]))\n",
        "print(word2tags['present'])\n",
        "word2tags['present'] = 'N'\n",
        "print('{}: {}'.format(word, word2tags[word]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jngBzF-_DO5d",
        "colab_type": "text"
      },
      "source": [
        "A better way is to use the mapping **(word (string), tags (list))** for the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTSrYWuHDO5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2tags = {'present': ['V', 'N'], 'object': ['V', 'N']}\n",
        "for k, v in word2tags.items():\n",
        "    print('{}: {}'.format(k, word2tags[k]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL6PySYSDO5j",
        "colab_type": "text"
      },
      "source": [
        "And if we want to further add a new tag to the word, we can simply `append` it to the tag list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gqs2ahODO5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = 'present'\n",
        "print('{}: {}'.format(word, word2tags[word]))\n",
        "word2tags[word].append('ADJ')\n",
        "print('{}: {}'.format(word, word2tags[word]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUZC5Nq6DO5p",
        "colab_type": "text"
      },
      "source": [
        "#### Default dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEk5Sgs9DO5q",
        "colab_type": "text"
      },
      "source": [
        "If we try to access a key that is not in a dictionary, we get an error. \n",
        "However, itâ€™s often useful if a dictionary can automatically create an entry for this new key and give it a default value, such as zero or the empty list.\n",
        "Both **NLTK** and **Python** provide us with the `defaultdict`, a more advanced dictionary, where we can supply a parameter which can be used to create the default value, e.g., int, float, str, list, dict, tuple.\n",
        "For simplicity, we will stick to the `defaultdict` of NLTK, and for that of Python you can refer to the [link](https://docs.python.org/3/library/collections.html#collections.defaultdict).\n",
        "\n",
        "Here we assign 0 frequency to a new tag *UNK*, and a tag list with the most frequent tag **N** inside it to a new word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kaLPKVfDO5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_fd_def = nltk.defaultdict(int, tag_fd)\n",
        "print(tag_fd_def['UNK'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b8doyuIDO5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2tags_def = nltk.defaultdict(lambda: ['N'], word2tags)\n",
        "print(word2tags_def['a_new_word'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkPsNCLRDO5z",
        "colab_type": "text"
      },
      "source": [
        "> **<h3>ðŸ’» Try it yourself!</h3>**\n",
        "\n",
        "1. In NLP, one of the preprocessing steps is to replace low-frequency words with a special \"out of vocabulary\" token, *UNK*, with the help of a default dictionary. \n",
        "Preprocess `alice` corpus: keep the most frequent *n* words, and map the rest words to UNK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTx4UBk1DO50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loHwiLMoDO55",
        "colab_type": "text"
      },
      "source": [
        "2. Create a POS unigram for the Penn Treebank Corpus of the mapping **(word, tag_list)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T71k7AOBDO55",
        "colab_type": "text"
      },
      "source": [
        "#### POS tag n-grams for tagging\n",
        "We can use default dictionaries with complex keys and values to build tag n-grams for tagging.\n",
        "\n",
        "Taking bi-gram as an example, we want to store the frequency of tag **t2**, which is dependent on the tag **t1** of its previous word, and the current word **w2**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxM-LjDWDO56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a nested default dictionary, where the value is another default dictionary whose default value is int (0)\n",
        "pos = nltk.defaultdict(lambda: nltk.defaultdict(int))\n",
        "# create bigrams of (word, tag) pair\n",
        "for ((w1, t1), (w2, t2)) in nltk.bigrams(nltk.corpus.treebank.tagged_words()):\n",
        "    pos[(t1, w2)][t2] += 1\n",
        "print(pos[('DT', 'right')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBUzcFILDO5_",
        "colab_type": "text"
      },
      "source": [
        "Each time through the loop we updated our `pos` entry for **(t1, w2)**, a tag and its following word. \n",
        "When we look up an item in pos we must specify a compound key , and we get back a dictionary object. \n",
        "A POS tagger could use such information to decide that the word *right*, when preceded by a determiner, should be tagged as one of the tags in *NN*, *JJ*, and *RB*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5k41padDO6A",
        "colab_type": "text"
      },
      "source": [
        "### Automatic Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdyqqkNcDO6B",
        "colab_type": "text"
      },
      "source": [
        "We will explore various ways to automatically perform POS tagging. We will see that the tag of a word depends on the word and its context within a *sentence*. \n",
        "For this reason, we will be working with data at the level of (tagged) sentences rather than words. \n",
        "\n",
        "Weâ€™ll begin by loading the data we will be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONunhs7PDO6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "penn_tagged_sents = nltk.corpus.treebank.tagged_sents()\n",
        "penn_sents = nltk.corpus.treebank.sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIlSUz_uDO6E",
        "colab_type": "text"
      },
      "source": [
        "#### Data Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znPR6DKVDO6F",
        "colab_type": "text"
      },
      "source": [
        "As we mentioned earlier, for most machine learning POS taggers, a split of the original dataset into the *training* set and *test* set is necessary.\n",
        "As Training and testing a model on a single dataset can easily lead to the problem of [overfitting](https://en.wikipedia.org/wiki/Overfitting), as a tagger that simply memorized its training data and made no attempt to construct a general model would get a perfect score, but would be useless for tagging new text. \n",
        "Therefore, another held-out test set is used after training the model with training set to evaluate the generalizability of the model.\n",
        "\n",
        "In this section, we split the data, training on 90% and testing on the remaining 10%, and this proportion may vary depending on the task and data size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq2hvscwDO6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_prop = 0.9\n",
        "size = int(len(penn_tagged_sents) * train_prop)\n",
        "train_tagged_sents = penn_tagged_sents[:size]\n",
        "test_tagged_sents = penn_tagged_sents[size:]\n",
        "train_sents = penn_sents[:size]\n",
        "test_sents = penn_sents[size:]\n",
        "print('Training sents: {} {}'.format(len(train_sents), len(train_tagged_sents)))\n",
        "print('Test sents: {} {}'.format(len(test_sents), len(test_tagged_sents)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAf_c5rfDO6Q",
        "colab_type": "text"
      },
      "source": [
        "Now we split the original annotated dataset into a training and test set. we will evaluate ALL taggers on the test set, and use training data for taggers that require data to train on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1xHxy7DO6S",
        "colab_type": "text"
      },
      "source": [
        "> **<h3>ðŸ’» Try it yourself!</h3>**\n",
        "\n",
        "1. Create the tag distribution plot of both *training* and *test* set of Penn Treebank Corpus, and compare them with the plot of the whole corpus. What difference can you see?\n",
        "2. Measure the out-of-vocabulary (OOV) rate between the training and test sets:\n",
        "    - report the number of tokens and token types in the 90% training set;\n",
        "    - report the number of tokens and token types in the 10% test set;\n",
        "    - report the TEST token OOV rate and token type OOV rate, i.e. how many of test that are not present in training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBaoeH0nDO6U",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8DdBzBZDO6U",
        "colab_type": "text"
      },
      "source": [
        "Evaluation is another core part of NLP.\n",
        "When evaluating our models on test set annotated by human experts, what numbers are we going to report? An intuitive one is the *accuracy* comparing the gold standard test set tags and the predicted tags produced by the model, i.e. the rate of correct tags the model has predicted.\n",
        "\n",
        "Recall the term *presicion*, *recall* and *F1-score* for 2-class classification in previous modules, it is still applicable in POS tagging, where we can have these number for EACH POS tag category (NN v.s. non NN). We can achieve this by building a *confusion matrix* of the outputs.\n",
        "\n",
        "Let's use the tagger in the very beginning of the section and create such confusion matrix:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avsdYKNeDO6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gold_test_tags = [tag for test_tagged_sent in test_tagged_sents for (word, tag) in test_tagged_sent]\n",
        "pred_test_tags = [tag for test_sent in test_sents for (word, tag) in nltk.pos_tag(test_sent)]\n",
        "print(nltk.ConfusionMatrix(gold_test_tags, pred_test_tags).pretty_format())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Andl53zVDO6X",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the row values indicate the POS tags given by reference (gold standard test set), and the column tags are from the model output.\n",
        "\n",
        "In order to calculate the metrics we mentioned earlier, let's first read the whole matrix in:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_iXYufyDO6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "cm = nltk.ConfusionMatrix(gold_tags, test_tags)\n",
        "cm_matrix = np.array(cm._confusion)\n",
        "print(cm_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ceg0Y4mQDO6f",
        "colab_type": "text"
      },
      "source": [
        "The total number of tags are just the sum of the matrix value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eF8yAI8DO6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_tags = np.sum(cm_matrix)\n",
        "total_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnA2NbNqDO6m",
        "colab_type": "text"
      },
      "source": [
        "*Accuracy* is simply the ratial between the correct tags (sum of the diagnoal value called *trace*) and total tag number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpT3mijBDO6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = np.trace(cm_matrix) / total_tags\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLR3CjKJDO6q",
        "colab_type": "text"
      },
      "source": [
        "Taking **NN** as an example, we want to calualte the *precision*, *recall* and *F1-score*.\n",
        "First we need to calculate true positive, false positive and false negative.\n",
        "True positive is simply the entry where both row and column are **NN**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sERKpF1dDO6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_idx = cm._indices['NN']\n",
        "tp = cm_matrix[nn_idx][nn_idx]\n",
        "tp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYamHpGWDO6v",
        "colab_type": "text"
      },
      "source": [
        "False positive is the sum of *NN* column values exluding true positive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HptD6yRpDO6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fp = sum(cm_matrix[:, nn_idx]) - tp\n",
        "fp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-ixUhdRDO6y",
        "colab_type": "text"
      },
      "source": [
        "Similarly, False negative is the sum of *NN* row values exluding true positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9anYdmXDO6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fn = sum(cm_matrix[nn_idx, :]) - tp\n",
        "fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84GxTQeTDO62",
        "colab_type": "text"
      },
      "source": [
        "The precision, recall and F1-score can be caclculated accordingly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S56TbzMWDO63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXq2EdxtDO68",
        "colab_type": "text"
      },
      "source": [
        "> **<h3>ðŸ’» Try it yourself!</h3>**\n",
        "\n",
        "1. Calculate the three metrics for each POS category with the same tagger, and complete the table below.\n",
        "\n",
        "| POS           | Precision     | Recall  | F1-score \n",
        "| ------------- |---------------|---------|----------\n",
        "| NN      | 0.86 | 0.93 | 0.89 \n",
        "| $       |      |      |      \n",
        "| ''      |      |      |      \n",
        "| Add tag here   |      |      |      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-fTGxzsDO69",
        "colab_type": "text"
      },
      "source": [
        "#### Default tagger\n",
        "The simplest possible tagger assigns the same tag to each token. This may seem to be a rather banal step, but it establishes an important baseline for tagger performance. \n",
        "In order to get the best result, we tag each word with the most likely tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5foR3yhNDO69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.FreqDist(gold_test_tags).max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25jgzTOaDO7B",
        "colab_type": "text"
      },
      "source": [
        "Now we can create a tagger that tags everything as **NN**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N-ARB82DO7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "default_preds = [(word, tag) for test_sent in test_sents for (word, tag) in default_tagger.tag(test_sent)]\n",
        "default_preds[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1SjegFcDO7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "The accuracy of this tagger is"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hsMtyVuDO7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_pred_tags = [tag for (word, tag) in default_preds]\n",
        "default_cm = nltk.ConfusionMatrix(gold_tags, default_pred_tags)\n",
        "default_cm_matrix = np.array(default_cm._confusion)\n",
        "acc = np.trace(default_cm_matrix) / np.sum(default_cm_matrix)\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8yoUd18DO7M",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, we can directly calculate accuracy with `evaluate()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtTPYChjDO7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_sys = default_tagger.evaluate(test_tagged_sents)\n",
        "acc_sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m9nU5CWDO7T",
        "colab_type": "text"
      },
      "source": [
        "> **<h3>ðŸ’» Try it yourself!</h3>**\n",
        "\n",
        "1. Calculate the three metrics for each POS category with the **default** tagger, and complete the table below.\n",
        "\n",
        "| POS           | Precision     | Recall  | F1-score \n",
        "| ------------- |---------------|---------|----------\n",
        "| NN      |  |  |  \n",
        "| $       |      |      |      \n",
        "| ''      |      |      |      \n",
        "| Add tag here   |      |      |      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fogy2wPBDO7V",
        "colab_type": "text"
      },
      "source": [
        "#### The most frequent 100 unigram tagger\n",
        "A lot of high-frequency words do not have the **NN** tag. \n",
        "Letâ€™s find the 100 most frequent words and store their most likely tag. We can then use this information as the\n",
        "model for a *lookup tagger*.\n",
        "\n",
        "START FROM HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvRyNfEGDO7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a FreqDist of word unigram from training corpus\n",
        "fd = nltk.FreqDist([word for train_tagged_sent in train_tagged_sents for (word, tag) in train_tagged_sent])\n",
        "# get the 100 most frequent unigram word \n",
        "most_freq_words = list(fd.keys())[:100]\n",
        "# create a ConditionalFreqDist associating the word and POS tags\n",
        "cfd = nltk.ConditionalFreqDist([(word, tag) for train_tagged_sent in train_tagged_sents for (word, tag) in train_tagged_sent])\n",
        "# get the most likely tag for each word\n",
        "likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n",
        "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
        "baseline_tagger.evaluate(test_tagged_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE7Pd4VgDO7a",
        "colab_type": "text"
      },
      "source": [
        "** The most frequent 100 unigram tagger\n",
        "Again, evaluate it using recall, precision and F1 for each category \n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "Again encourage the students to look and learn from the evaluation data\n",
        "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "** A simple bi-gram tagger\n",
        "Emphasize that a real n-gram tagger would do decoding using a dynamic programming algorithm called Viterbi (the students will have heard of this in the lecture).  Today in the interests of time we are not going to do decoding, weâ€™re just going to assign the most frequent POS sequence to each bigram that matches (if indeed a bigram does match).\n",
        "Again, evaluate it using recall, precision and F1 for each category \n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "Low accuracy because of data sparsity â€“ most bigrams were never seen during training.\n",
        "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
        "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
        "** Combining a models with backoff (e.g. unigram plus bigram tagger)\n",
        "** Further insights from evaluation using a confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQUEHuepDO7b",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwqBT8ggDO7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX-V_CfgDO7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoDGtjPsDO7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8x16dqODO7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htf_v_H9DO7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhU_uUjGDO7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lwZwEO_DO7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('hello world!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tikKx6L7DO71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int(3.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqCUL5wHDO74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len([1,2,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-uPLmi3DO78",
        "colab_type": "text"
      },
      "source": [
        "Another similar concept is `method`. Check [here](https://stackoverflow.com/questions/155609/whats-the-difference-between-a-method-and-a-function) for their difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpIUHGCYDO7-",
        "colab_type": "text"
      },
      "source": [
        "Now let's learn how to define and call functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNT4jm13DO8A",
        "colab_type": "text"
      },
      "source": [
        "### Define a function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxMq4oKLDO8A",
        "colab_type": "text"
      },
      "source": [
        "A function is defined by using the `def` keyword, followed by \n",
        "1. a **function name** of your choosing;\n",
        "2. a set of parentheses which hold any **parameters** the function will take (they can be empty);\n",
        "3. an ending **colon**;\n",
        "4. function **content code**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ0qDu8dDO8B",
        "colab_type": "text"
      },
      "source": [
        "Let's define a simple function `hello()`that prints **hello world**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoxADFWQDO8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hello():\n",
        "    print('hello world')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPCj9Wr0DO8G",
        "colab_type": "text"
      },
      "source": [
        "Note that \n",
        "- We do not have any parameter for this function, and we will discuss on it in more details later;\n",
        "- To start our real function content code, python requires a 4-space indent, as shown above.\n",
        "\n",
        "Now we have defined our first function. In order to let the program run this function, we still need to call the function, just like what we did to the python built-in functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEvNdM6FDO8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hello()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BgYAh3qDO8K",
        "colab_type": "text"
      },
      "source": [
        "Functions can be more complicated. For example, we can use `for` loops, conditional statements, and more within our function block.\n",
        "\n",
        "For example, the function defined below utilizes a conditional statement to check if the input for the name variable contains a vowel, then uses a for loop to iterate over the letters in the name string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIdvp0J0DO8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define function names()\n",
        "def names():\n",
        "    # Set up name variable with input\n",
        "    name = str(input('Enter your name: '))\n",
        "    # Check whether name has a vowel\n",
        "    if set('aeiou').intersection(name.lower()):\n",
        "        print('Your name contains a vowel.')\n",
        "    else:\n",
        "        print('Your name does not contain a vowel.')\n",
        "\n",
        "    # Iterate over name\n",
        "    for letter in name:\n",
        "        print(letter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOM-WhpgDO8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call the function\n",
        "names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTdB3LX3DO8S",
        "colab_type": "text"
      },
      "source": [
        "The `names()` function sets up a conditional statement and a for loop, showing how code can be organized within a function definition.\n",
        "\n",
        "> Defining functions within a program makes our code modular and reusable so that we can call the same functions without rewriting them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQA5-2ukDO8S",
        "colab_type": "text"
      },
      "source": [
        "### Function parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "645xwVkWDO8S",
        "colab_type": "text"
      },
      "source": [
        "So far we have looked at functions with empty parentheses, but we can define parameters in function definitions within their parentheses.\n",
        "\n",
        "> A parameter is a variable in the definition of a function that the function can accept.\n",
        "\n",
        "Letâ€™s create a function that takes trhee parameters `x`, `y`, `z`, and adds them in different configurations. The sums of these will be printed by the function. Then weâ€™ll call the function and pass numbers into the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3HofhKfDO8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_numbers(x, y, z):\n",
        "    a = x + y\n",
        "    b = x + z\n",
        "    c = y + z\n",
        "    print(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVX80wVMDO8Z",
        "colab_type": "text"
      },
      "source": [
        "Now we can pass the `arguments` we want into the function to call it. Check [here](https://www.quora.com/What-is-the-difference-between-argument-and-parameters-in-C) for the difference between parameter and argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz19CAiTDO8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_numbers(1, 2, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH9c_oBjDO8c",
        "colab_type": "text"
      },
      "source": [
        "We passed the number `1` in for the `x` parameter, `2` in for the `y` parameter, and `3` in for the `z` parameter. These values correspond with each parameter in the order they are given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzsZFPZVDO8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try different arguments\n",
        "add_numbers(4, 6, 8)\n",
        "add_numbers('a', 'b', 'c') # how does it work?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "oTly1fTjDO8g",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:1.5em; font-weight: bold\">\n",
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/exercise.png?raw=1\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
        "Try it yourself! <hr>\n",
        "</p>\n",
        "\n",
        "Write a function that, given an input list of integers, prints their average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t_p9Q_4DO8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xco4PIByDO8i",
        "colab_type": "text"
      },
      "source": [
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p><code>\n",
        "  def get_average_list(l):\n",
        "      print( sum(l) / len(l) )\n",
        "  </code></p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEDdfHc1DO8j",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:1.5em; font-weight: bold\">\n",
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/exercise.png?raw=1\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
        "Try it yourself! <hr>\n",
        "</p>\n",
        "\n",
        "Write a function that, given an input string, computes and prints the following statistics:\n",
        "- Total number of characters.\n",
        "- Total number of tokens.\n",
        "- Total number of word types (i.e., distinct tokens) *Hint: there is a datatype that can help you with this!*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM04Ys7CDO8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_statistics(s):\n",
        "    tot_characters = #TODO\n",
        "    tot_tokens = #TODO\n",
        "    tot_types = #TODO\n",
        "    \n",
        "    print(\"The given input string contains\", tot_characters, \"characters,\",\\\n",
        "         tot_tokens, \"tokens and\", tot_types, \"types\")\n",
        "\n",
        "# Test your function here!\n",
        "test_string = \"To be or not to be, not to be or to be!\"\n",
        "string_statistics(test_string) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z437syK4DO8n",
        "colab_type": "text"
      },
      "source": [
        "### Keyword Arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvUzyfeYDO8o",
        "colab_type": "text"
      },
      "source": [
        "We can also use keyword arguments in a function call to identify the arguments by the parameter name.\n",
        "\n",
        "When using keyword arguments, we can use parameters out of order because the Python interpreter will use the keywords provided to match the values to the parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMAJLO2UDO8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_numbers(x = 4, y = 6, z = 8)\n",
        "add_numbers(y = 6, x = 4, z = 8) # out of order, but same arguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIKjlUWWDO8q",
        "colab_type": "text"
      },
      "source": [
        "### Default Argument Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EowavdBzDO8q",
        "colab_type": "text"
      },
      "source": [
        "We can also provide default values for one or more parameters, so the parameters will be set to the default values\n",
        "if we do not mention them when calling the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzvAyOSkDO82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_numbers_default_value(x, y = 6, z = 10):\n",
        "    a = x + y\n",
        "    b = x + z\n",
        "    c = y + z\n",
        "    print(a, b, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpABbtY9DO85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_numbers_default_value(4)\n",
        "add_numbers_default_value(x = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu3jprhBDO86",
        "colab_type": "text"
      },
      "source": [
        "We can also explicitly pass the arguments to the parameters with the default value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lDNEVeADO87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_numbers_default_value(4, y = 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QImF_ptDO89",
        "colab_type": "text"
      },
      "source": [
        "### Returning a Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IhgyQGJDO89",
        "colab_type": "text"
      },
      "source": [
        "We can pass parameters into a function, and a function can also return a value to us in the end. When a function exits, it can *optionally* pass an expression back to the caller, with the `return` statement. If you use a `return` statement with no arguments, the function will return `None`.\n",
        "\n",
        "So far, we have used the `print()` statement instead of the return statement in our functions. Letâ€™s create a program that instead of printing will return a variable. The function `square` will take a parameter `x`, and returns the variable `y` representing the square of `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlUXbUmvDO8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def square(x):\n",
        "    y = x ** 2\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uft3D5rRDO9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = square(3)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTiPf5YdDO9D",
        "colab_type": "text"
      },
      "source": [
        "As stated previously, we can use `return` with no arguments, so that the function will return `None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwU_WnGvDO9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we return no arguments\n",
        "def square_return_noarg(x):\n",
        "    y = x ** 2\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieFV28rwDO9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = square_return_noarg(3)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYNnRkFODO9I",
        "colab_type": "text"
      },
      "source": [
        "Additionally, without using the `return` statement here, the function cannot return a value so the value defaults to `None` as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uMZokpiDO9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we do no use return\n",
        "def square_noreturn(x):\n",
        "    y = x ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdyH_753DO9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = square_noreturn(3)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmZuLQ7aDO9O",
        "colab_type": "text"
      },
      "source": [
        "In the previous function `add_numbers`, instead of printing the results, we can `return` more than one value wrapped in data structures such as **tuples**, **lists** or **dictionaries**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpzFzioMDO9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_numbers_return_dict(x, y, z):\n",
        "    a = x + y\n",
        "    b = x + z\n",
        "    c = y + z\n",
        "    return {'a': a, 'b': b, 'c': c}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkr5j7ynDO9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = add_numbers_return_dict(1, 2, 3)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm4y8daUDO9T",
        "colab_type": "text"
      },
      "source": [
        "Whenever the program hits a `return` statement, the function will exit immediately, whether or not they are returning a value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDp5TlL5DO9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loop_five():\n",
        "    for x in range(0, 25):\n",
        "        print(x)\n",
        "        if x == 5:\n",
        "            # Stop function at x == 5\n",
        "            return\n",
        "    print(\"This line will not execute.\")\n",
        "\n",
        "loop_five()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hihlE7ZxDO9X",
        "colab_type": "text"
      },
      "source": [
        "The function hits `return` before the `for` loop ends, so the line that is outside of the loop will not run. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUp83FPUDO9X",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:1.5em; font-weight: bold\">\n",
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/exercise.png?raw=1\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
        "Try it yourself! <hr>\n",
        "</p>\n",
        "\n",
        "Write a function that:\n",
        "- Takes as input a shopping list in the form of `items : quantity` pairs;\n",
        "- For each item to buy, prints a reminder in the form: *Don't forget to buy* + `quantity` + `items`;\n",
        "- Ends by wishing you a nice shopping time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8RortNfDO9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_shopping_list(shopping_list):\n",
        "    # write here your function\n",
        "    pass\n",
        "\n",
        "# Test your function here!\n",
        "shopping_list = {\"pears\": 5, \"apples\": 7, \"bananas\": 4}\n",
        "print_shopping_list(shopping_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXjNpSlzDO9f",
        "colab_type": "text"
      },
      "source": [
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p><code>\n",
        "    def print_shopping_list(shopping_list):\n",
        "        for item, quantity in shopping_list.items():\n",
        "            print(\"Don't forget to buy\", quantity, item)\n",
        "        print(\"Have a nice shopping time!\")\n",
        "  </code></p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbCOa-LKDO9g",
        "colab_type": "text"
      },
      "source": [
        "Now, let's make our function a bit nicer.\n",
        "What would happen, for example, if someone would enter an incorrect input?\n",
        "Our function would crash!\n",
        "\n",
        "Let's improve our `print_shopping_list` function in the cell above in this way:\n",
        "- At the beginning of the function, check that the given input is of type `dict`;\n",
        "- If the input doesn't pass this test, return a `print` statement that asks the user for a correct input.\n",
        "\n",
        "Now, test our improved funtion with the test inputs below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxh_u6CdDO9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test your improved function here!\n",
        "print_shopping_list({\"pears\": 5, \"apples\": 7, \"bananas\": 4})\n",
        "print_shopping_list([\"pears\", \"apples\", \"bananas\"])\n",
        "print_shopping_list({\"tomatoes\": 9, \"cat_litter\": 1, \"thuna_chunks\": 4})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRNk1mkNDO9i",
        "colab_type": "text"
      },
      "source": [
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p><code>\n",
        "    def print_shopping_list(shopping_list):\n",
        "        if type(shopping_list) != dict:\n",
        "            return(\"Please, give me a valid shopping list!\")\n",
        "        for item, quantity in shopping_list.items():\n",
        "            print(\"Don't forget to buy\", quantity, item)\n",
        "        print(\"Have a nice shopping time!\")\n",
        "  </code></p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhjunpVqDO9i",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:1.5em; font-weight: bold\">\n",
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/exercise.png?raw=1\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
        "Try it yourself! <hr>\n",
        "</p>\n",
        "\n",
        "Now, let's write a function that returns the total cost of a shopping list given as inputs two dictionaries: a `shopping_list` dictionary composed of `item : quantity` pairs (as in the excercise above) and an `inventory` dictionary composed of `item : cost` pairs.\n",
        "\n",
        "\n",
        "Complete the function below with the missing statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TY-CsYGDO9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_shopping_bill(shopping_list, inventory):\n",
        "    shopping_cost = 0\n",
        "    for item, quantity in shopping_list.items():\n",
        "        # TODO: get the total price by summing up the prices of single items\n",
        "\n",
        "    return shopping_cost\n",
        "        \n",
        "    \n",
        "# Test your function here!\n",
        "our_shopping = {\"pears\": 5, \"apples\": 7, \"bananas\": 4}\n",
        "current_inventory = {\"pears\": 0.5, \"apples\": 0.7, \"bananas\": 1.3}\n",
        "\n",
        "cost = calculate_shopping_bill(our_shopping, current_inventory)\n",
        "print(\"You'll spend\", cost, \"GBP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cPgL7duwDO9l",
        "colab_type": "text"
      },
      "source": [
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p><code>\n",
        "    def calculate_shopping_bill(shopping_list, inventory):\n",
        "        shopping_cost = 0\n",
        "        for item, quantity in shopping_list.items():\n",
        "            single_item_cost = inventory[item]\n",
        "            total_item_cost = single_item_cost * quantity\n",
        "            shopping_cost += total_item_cost\n",
        "        return shopping_cost\n",
        "  </code></p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdP-hCqiDO9l",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/quiz.png?raw=1\" style=\"height:36pt; float:left; margin-right: 4pt\" /> <h2>Quiz</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qU5M_FYDO9m",
        "colab_type": "text"
      },
      "source": [
        "Given the function:\n",
        "```\n",
        "def add_numbers(x = 3, y = 4, z = 5):\n",
        "    x = x + y\n",
        "    y = x + z\n",
        "    z = y + z\n",
        "    print(x, y, z)\n",
        "    \n",
        "o, p, q = 5, 6, 7\n",
        "L = [5,6,7]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AI1mDDqDO9n",
        "colab_type": "text"
      },
      "source": [
        "1. What does the following code print?\n",
        "\n",
        "```\n",
        "d = add_numbers(5)\n",
        "```\n",
        "\n",
        "A. 7, 8, 9\n",
        "\n",
        "B. 7, 8, 9\n",
        "\n",
        "C. 9, 14, 19\n",
        "\n",
        "D. 9, 14, 19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-_QxkARDO9n",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>C</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St9y0iTYDO9o",
        "colab_type": "text"
      },
      "source": [
        "2. What is the value of `d`?\n",
        "\n",
        "A. 7\n",
        "\n",
        "B. 9\n",
        "\n",
        "C. 10\n",
        "\n",
        "D. None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4evHqvn7DO9o",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>D</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OPm8DNbDO9o",
        "colab_type": "text"
      },
      "source": [
        "3. What is the value of `o p q` after running the following code?\n",
        "\n",
        "```\n",
        "add_numbers(o, p, q)\n",
        "```\n",
        "\n",
        "A. 7, 12, 17\n",
        "\n",
        "B. 9, 14, 19\n",
        "\n",
        "C. 11, 18, 25\n",
        "\n",
        "D. 5, 6, 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq8O_Jt3DO9p",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>D</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7xeDdktDO9p",
        "colab_type": "text"
      },
      "source": [
        "4. How to pass the element of `L` as the parameters of the function?\n",
        "\n",
        "\n",
        "A. ```add_numbers(L)```\n",
        "\n",
        "\n",
        "B. ```add_numbers(L*)```\n",
        "\n",
        "C. ```add_numbers(*L)```\n",
        "\n",
        "D. ```add_numbers(**L)```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9D1Z_7DO9p",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>C</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TftOVem5DO9p",
        "colab_type": "text"
      },
      "source": [
        "5. What is the value of `L` after running the following code?\n",
        "\n",
        "```\n",
        "add_numbers(L, L, L)\n",
        "```\n",
        "\n",
        "A. [5, 6, 7]\n",
        "\n",
        "B. [5, 6, 7, 5, 6, 7]\n",
        "\n",
        "C. [5, 6, 7, 5, 6, 7, 5, 6, 7]\n",
        "\n",
        "D. [5, 6, 7, 5, 6, 7, 5, 6, 7, 5, 6, 7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQwH8nSiDO9p",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>A</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BND-Y9zDO9r",
        "colab_type": "text"
      },
      "source": [
        "<h1 align='center' style='margin-top:2em'>\n",
        "    <img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/section_header.png?raw=1\" \n",
        "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
        "    <u>Regular Expressions</u>\n",
        "    <img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/section_header.png?raw=1\" \n",
        "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
        "<hr>\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2df2eqZiDO9r",
        "colab_type": "text"
      },
      "source": [
        "This section is heavily built on this [tutorial](https://docs.python.org/3.6/howto/regex.html#regex-howto), and students are strongly recommended to go through it. A more comprehensive documentation of python regular expressions can be found [here](https://docs.python.org/3.6/library/re.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEL0TT9hDO9r",
        "colab_type": "text"
      },
      "source": [
        "Regular expressions (called REs, or regexes, or regex patterns) are essentially a tiny, highly specialized programming language embedded inside Python and made available through the re module. Using this little language, you specify the rules for the set of possible strings that you want to match; this set might contain English sentences, or e-mail addresses, or TeX commands, or anything you like. You can then ask questions such as â€œDoes this string match the pattern?â€, or â€œIs there a match for the pattern anywhere in this string?â€. You can also use REs to modify a string or to split it apart in various ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_T_Hey1DO9r",
        "colab_type": "text"
      },
      "source": [
        "Python uses the **raw string** notation for RE patterns, and backslashes `'\\'` are not handled in any special way in a string literal prefixed with `r`. So `r'\\n'` is a two-character string containing `'\\'` and `'n'`, while `'\\n'` is a one-character string containing a newline. \n",
        "\n",
        "Weâ€™ll use the raw string notation for the rest of the section. We will also write REs in highlight style , i.e. `r'\\n'` is equivalent to `\\n`, usually without quotes, and strings to be matched 'in single quotes'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BovHhtt7DO9r",
        "colab_type": "text"
      },
      "source": [
        "REs can contain both special and ordinary characters. Most ordinary characters, like `A`, `a`, or `0`, are the simplest REs; they simply match themselves. You can concatenate ordinary characters to match more complex sequence. For example, `test` will match the string 'test' exactly.\n",
        "\n",
        "Some characters are special metacharacters, and donâ€™t match themselves. Instead, they signal that some out-of-the-ordinary thing should be matched, or they affect other portions of the RE by repeating them or changing their meaning.\n",
        "\n",
        "Hereâ€™s a complete list of the metacharacters and their explanations:\n",
        "\n",
        "- `.`             Matches any character except a newline.\n",
        "- `^`             Matches the start of the string.\n",
        "- `$`             Matches the end of the string or just before the newline at the end of the string.\n",
        "- `*`             Matches 0 or more (greedy) repetitions of the preceding RE. Greedy means that it will match as many repetitions as possible.\n",
        "- `+`             Matches 1 or more (greedy) repetitions of the preceding RE.\n",
        "- `?`             Matches 0 or 1 (greedy) of the preceding RE.\n",
        "- `*?, +?, ??`    Non-greedy versions of the previous three special characters.\n",
        "- `{m,n}`         Matches from m to n repetitions of the preceding RE.\n",
        "- `{m,n}?`        Non-greedy version of the above.\n",
        "- `\\\\`            Either escapes special characters or signals a special sequence.\n",
        "- `[]`            Indicates a set of characters. A \"^\" as the first character indicates a complementing set.\n",
        "- `|`             A|B, creates an RE that will match either A or B.\n",
        "- `(...)`         Matches the RE inside the parentheses. The contents can be retrieved or matched later in the string.\n",
        "\n",
        "\n",
        "The special sequences consist of \"\\\\\" and a character from the list below:\n",
        "- `\\number`  Matches the contents of the group of the same number.\n",
        "- `\\A`       Matches only at the start of the string.\n",
        "- `\\Z`       Matches only at the end of the string.\n",
        "- `\\b`       Matches the empty string, but only at the start or end of a word.\n",
        "- `\\B`       Matches the empty string, but not at the start or end of a word.\n",
        "- `\\d`       Matches any decimal digit; equivalent to the set `[0-9]`.\n",
        "- `\\D`       Matches any non-digit character; equivalent to `[^\\d]`.\n",
        "- `\\s`       Matches any whitespace character; equivalent to `[ \\t\\n\\r\\f\\v]`.\n",
        "- `\\S`       Matches any non-whitespace character; equivalent to `[^\\s]`.\n",
        "- `\\w`       Matches any alphanumeric character; equivalent to `[a-zA-Z0-9_]`.\n",
        "- `\\W`       Matches the complement of `\\w`.\n",
        "- `\\\\`       Matches a literal backslash."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG2NDw7RDO9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import built-in regular expression module in python\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmgyMopcDO9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = (\"\"\"                2\n",
        "  When forty winters shall besiege thy brow,\"\"\"\n",
        "  \"And dig deep trenches in thy beauty's field, \"\n",
        "  \"Thy youth's proud livery so gazed on now, \"\n",
        "  \"Will be a tattered weed of small worth held: \"\n",
        "  \"Then being asked, where all thy beauty lies, \"\n",
        "  \"Where all the treasure of thy lusty days; \"\n",
        "  \"To say within thine own deep sunken eyes, \"\n",
        "  \"Were an all-eating shame, and thriftless praise. \"\n",
        "  \"How much more praise deserved thy beauty's use, \"\n",
        "  \"If thou couldst answer 'This fair child of mine \"\n",
        "  \"Shall sum my count, and make my old excuse' \"\n",
        "  \"Proving his beauty by succession thine. \"\n",
        "  \"This were to be new made when thou art old, \"\n",
        "  \"And see thy blood warm when thou feel'st it cold.\")\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64yLveRUDO9w",
        "colab_type": "text"
      },
      "source": [
        "We will mainly concentrate two essential usage of regular expressions: **searching** and **substituting** text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAwJNW8FDO9w",
        "colab_type": "text"
      },
      "source": [
        "### Searching text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdmZQroDO9x",
        "colab_type": "text"
      },
      "source": [
        "Let's first find word '**where**', in regardless of case.\n",
        "\n",
        "The **re** module provides an interface to the regular expression engine, allowing you to compile REs into objects and then perform matches with them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtPJl7gDO9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = re.compile(r'\\b[Ww]here\\b')\n",
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkEmrYpVDO9z",
        "colab_type": "text"
      },
      "source": [
        "After we obtained compiled pattern object, it has several functions and attributes. For **searching** text, there are mainly four functions available:\n",
        "- *match()*: determine if the RE matches at the beginning of the string.\n",
        "- *search()*: scan through a string, looking for any location where this RE matches.\n",
        "- *findall()*: find all substrings where the RE matches, and returns them as a list.\n",
        "- *finditer()*: find all substrings where the RE matches, and returns them as an iterator.\n",
        "\n",
        "Please consult the **re** documentation for a complete listing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G2pcdz0DO90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = p.match(text)\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v_sLTwJDO92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = p.search(text)\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysJ7xaXPDO94",
        "colab_type": "text"
      },
      "source": [
        "The functions *match()* and *search()* return None if no match can be found. If theyâ€™re successful, a match object instance is returned, containing information about the match: where it starts and ends, the substring it matched, and more.\n",
        "\n",
        "In this example, as the text doesn't start with the word 'where', so *match()* won't find any match, whereas *search()* will look for any location where RE matches.\n",
        "\n",
        "We can further query the match object for information about the matching string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8RgeFXKDO95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m.group()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZXUCRaZDO98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m.start(), m.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eQOubF8DO-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m.span()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrGbTAsDO-B",
        "colab_type": "text"
      },
      "source": [
        "Now what if we want to find all matches, i.e. all words 'where'? We can use *findall()* and *finditer()*.\n",
        "\n",
        "The *findall()* returns a list of matching strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zODj3uZhDO-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = p.findall(text)\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MSVdzQIDO-E",
        "colab_type": "text"
      },
      "source": [
        "The *finditer()* returns a sequence of match object instances as an iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l68ZWBJrDO-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator = p.finditer(text)\n",
        "for match in iterator:\n",
        "    print(match)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2qxl7N8DO-G",
        "colab_type": "text"
      },
      "source": [
        "You donâ€™t have to create a pattern object and call its functions; the **re** module also provides the same top-level functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp3mao3YDO-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(re.search(r'\\b[Ww]here\\b', text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ0Xa0u4DO-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(re.findall(r'\\b[Ww]here\\b', text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy_ttiO4DO-L",
        "colab_type": "text"
      },
      "source": [
        "Notice that when we compile the RE, there is a **re.UNICODE** item when we print the object. It is a compliation flag which let you modify some aspects of how RE works. The full flag list is available in the module documentation. For example, we can use **re.IGNORECASE** or **re.I** to do case-insensitive matches, so that we do not have to take care of capital letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9NQpN1nDO-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = re.compile(r'\\bwhere\\b', re.I)\n",
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE2uJ32iDO-O",
        "colab_type": "text"
      },
      "source": [
        "And we will still have the same results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX5qqkh4DO-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = p.findall(text)\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJy39wbrDO-P",
        "colab_type": "text"
      },
      "source": [
        "Now try to come up with your regular expressions and search in the text to see if they will work.\n",
        "\n",
        "You can use the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQjWT6koDO-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSjim_0xDO-S",
        "colab_type": "text"
      },
      "source": [
        "#### Groupings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gYhZHpoDO-S",
        "colab_type": "text"
      },
      "source": [
        "Frequently you need to obtain more information than just whether the RE matched or not. REs are often used to dissect strings by writing a RE divided into several subgroups which match different components of interest.\n",
        "\n",
        "Groups are marked by the `'('`, `')'` metacharacters. They group together the expressions contained inside them, and you can repeat the contents of a group with a repeating qualifier, such as `*`, `+`, `?`, or `{m,n}`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQeSMdilDO-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = re.compile('((a(b)c)d)*')\n",
        "m = p.match('abcdabcd')\n",
        "print(m.span())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGDQlpGtDO-V",
        "colab_type": "text"
      },
      "source": [
        "Groups indicated with `'('`, `')'` also capture the starting and ending index of the text that they match; this can be retrieved by passing an argument to `group()`, `start()`, `end()`, and `span()`. \n",
        "\n",
        "Groups are numbered starting with 0. Group 0 is always present; itâ€™s the whole RE, so previous methods all have group 0 as their default argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4B3QjT4DO-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(m.start(0), m.end(0))\n",
        "print(m.group(0,1,2,3))\n",
        "print(m.span(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xrCEdFODO-X",
        "colab_type": "text"
      },
      "source": [
        "The `groups()` method returns a tuple containing the strings for all the subgroups, from 1 up to however many there are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMWBbgkpDO-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(m.groups())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46MjzYpDDO-a",
        "colab_type": "text"
      },
      "source": [
        "### Substituting text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G57ACQopDO-a",
        "colab_type": "text"
      },
      "source": [
        "We will use the following functions for substitution:\n",
        "- *sub()*: find all substrings where the RE matches, and replace them with a different string.\n",
        "- *subn()*: does the same thing as sub(), but returns the new string and the number of replacements.\n",
        "\n",
        "Again, you are encouraged to refer to the docs for more functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECc0ikEQDO-a",
        "colab_type": "text"
      },
      "source": [
        "***sub*** *(replacement, string[, count=0])*\n",
        "\n",
        "Returns the string obtained by replacing the leftmost non-overlapping occurrences of the RE in *string* by the replacement *replacement*. If the pattern isnâ€™t found, string is returned unchanged.\n",
        "\n",
        "The optional argument *count* is the maximum number of pattern occurrences to be replaced; *count* must be a non-negative integer. The default value of 0 means to replace all occurrences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk9xfbxCDO-b",
        "colab_type": "text"
      },
      "source": [
        "For the same example, let's substitute all `'where'` words to `'-------'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k8wEKyFDO-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = re.compile(r'\\b[Ww]here\\b')\n",
        "print(p.sub('------', text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeqPCAk-DO-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(p.sub('------', text, count = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2FIm2B_DO-e",
        "colab_type": "text"
      },
      "source": [
        "The `subn()` method does the same work, but returns a 2-tuple containing the new string value and the number of replacements that were performed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds2sGrP7DO-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(p.subn('------', text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWjCSc8KDO-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = re.compile('section{ (?P<name> [^}]* ) }', re.VERBOSE)\n",
        "p.sub(r'subsection{\\g<1>}','section{First} section{second}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPRLf_wODO-i",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:1.5em; font-weight: bold\">\n",
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/exercise.png?raw=1\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
        "Try it yourself! <hr>\n",
        "</p>\n",
        "\n",
        "Now it's your turn! Please write a regular expression to match sentences that start with the word â€œThisâ€ (case insensitive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_LycvwKDO-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3rj6SVcDO-l",
        "colab_type": "text"
      },
      "source": [
        "And a regex that matches words which are emoticons (like *:)*). At least 5 emoticons must be matched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SgRH9TCDO-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goe7ltNnDO-m",
        "colab_type": "text"
      },
      "source": [
        "And a regex that can match words which contains at least 3 *o*s in a row (so that *cooooool* would be a match, but *book* would not)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubjMbh8NDO-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBq9mIwHDO-o",
        "colab_type": "text"
      },
      "source": [
        "Finally, write a regex that substitutes any word-ending in *a* or *as* into an *o* or *os*.\n",
        "\n",
        "Test your regex by converting the Spanish sentence *Maria es ecuatoriana y tiene dos hijas rubias y muy altas* (Mary is Ecuadorian and has two blonde and very tall daughters) into its masculine equivalent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z52fI5KjDO-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeqKr0jLDO-r",
        "colab_type": "text"
      },
      "source": [
        "### Building a simple Eliza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kVW9lPqDO-r",
        "colab_type": "text"
      },
      "source": [
        "Having learnt about Eliza in the lecture, which is meant to emulate a Rogerian psychologist, let us now build a simple version of Eliza with RE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFf6Egq6DO-s",
        "colab_type": "text"
      },
      "source": [
        "Firstl, let us build a dictionary `grefs` that is used to map first-person pronouns to second-person pronouns and vice-versa. It is used to â€œreflectâ€ a statement back against the user:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baVxkPhqDO-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grefs = {\n",
        "    \"am\":          \"are\",\n",
        "    \"was\":         \"were\",\n",
        "    \"i\":           \"you\",\n",
        "    \"i'd\":         \"you would\",\n",
        "    \"i've\":        \"you have\",\n",
        "    \"i'll\":        \"you will\",\n",
        "    \"my\":          \"your\",\n",
        "    \"are\":         \"am\",\n",
        "    \"you are\":     \"I am\",\n",
        "    \"you've\":      \"I have\",\n",
        "    \"you'll\":      \"I will\",\n",
        "    \"your\":        \"my\",\n",
        "    \"yours\":       \"mine\",\n",
        "    \"you\":         \"me\",\n",
        "    \"me\":          \"you\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O03JVjbmDO-8",
        "colab_type": "text"
      },
      "source": [
        "Then we need another table `gpats` that is made up of a list of lists, where the first element is a RE that matches the userâ€™s statements and the second element is a list of potential responses. \n",
        "\n",
        "Many of the potential responses contain placeholders that can be filled in with fragments to echo the userâ€™s statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y75UoqDIDO-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpats = [\n",
        "  [r'I need (.*)',\n",
        "  [  \"Why do you need {0}?\",\n",
        "    \"Would it really help you to get {0}?\",\n",
        "    \"Are you sure you need {0}?\"]],\n",
        "\n",
        "  [r'I can\\'?t (.*)',\n",
        "  [  \"How do you know you can't {0}?\",\n",
        "    \"Perhaps you could {0} if you tried.\",\n",
        "    \"What would it take for you to {0}?\"]],\n",
        "\n",
        "  [r'I am (.*)',\n",
        "  [  \"Did you come to me because you are {0}?\",\n",
        "    \"How long have you been {0}?\",\n",
        "    \"How do you feel about being {0}?\"]],\n",
        "\n",
        "  [r'I\\'?m (.*)',\n",
        "  [  \"How does being {0} make you feel?\",\n",
        "    \"Do you enjoy being {0}?\",\n",
        "    \"Why do you tell me you're {0}?\",\n",
        "    \"Why do you think you're {0}?\"]],\n",
        "\n",
        "  [r'Are you ([^\\?]*)\\??',\n",
        "  [  \"Why does it matter whether I am {0}?\",\n",
        "    \"Would you prefer it if I were not {0}?\",\n",
        "    \"Perhaps you believe I am {0}.\",\n",
        "    \"I may be {0} -- what do you think?\"]],\n",
        "\n",
        "  [r'What (.*)',\n",
        "  [  \"Why do you ask?\",\n",
        "    \"How would an answer to that help you?\",\n",
        "    \"What do you think?\"]],\n",
        "\n",
        "  [r'How (.*)',\n",
        "  [  \"How do you suppose?\",\n",
        "    \"Perhaps you can answer your own question.\",\n",
        "    \"What is it you're really asking?\"]],\n",
        "\n",
        "  [r'Because (.*)',\n",
        "  [  \"Is that the real reason?\",\n",
        "    \"What other reasons come to mind?\",\n",
        "    \"Does that reason apply to anything else?\",\n",
        "    \"If {0}, what else must be true?\"]],\n",
        "\n",
        "  [r'Hello(.*)',\n",
        "  [  \"Hello... I'm glad you could drop by today.\",\n",
        "    \"Hi there... how are you today?\",\n",
        "    \"Hello, how are you feeling today?\"]],\n",
        "\n",
        "  [r'quit',\n",
        "  [  \"Thank you for talking with me.\",\n",
        "    \"Good-bye.\",\n",
        "    \"Thank you, that will be $150.  Have a good day!\"]],\n",
        "  \n",
        "  [r'(.*)',\n",
        "  [  \"Please tell me more.\",\n",
        "    \"Let's change focus a bit... Tell me about your family.\",\n",
        "    \"Can you elaborate on that?\",\n",
        "    \"Why do you say that {0}?\",\n",
        "    \"I see.\",\n",
        "    \"Very interesting.\",\n",
        "    \"{0}.\",\n",
        "    \"I see.  And what does that tell you?\",\n",
        "    \"How does that make you feel?\",\n",
        "    \"How do you feel when you say that?\"]] \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjV7obDkDO_A",
        "colab_type": "text"
      },
      "source": [
        "Now let us write the function `respond` that reads the user input and generates the response accordingly.\n",
        "\n",
        "We iterate through the regular expressions in `gpats`, trying to match each one with the userâ€™s input `s`. If we find a match, we choose a response template randomly from the list of possible responses associated with the matching pattern. Then we interpolate the match groups from the RE into the response string, calling the `transalte` function on each match group first.\n",
        "\n",
        "When we use the list comprehension to generate a list of reflected match groups, we explode the list with the asterisk `\\*` character before passing it to the stringâ€™s `format` method. Format expects a series of positional arguments corresponding to the number of format placeholders â€“ `{0}`, `{1}`, etc. â€“ in the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMGn_KkKDO_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import random\n",
        "def respond(s):\n",
        "    for pattern, responses in gpats:\n",
        "        # find a match for s\n",
        "        match = re.match(pattern, s)\n",
        "        if match:\n",
        "            # chosen randomly from among the available options\n",
        "            response = random.choice(responses)\n",
        "            return response.format(*[translate(g) for g in match.groups()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX7ooTZVDO_C",
        "colab_type": "text"
      },
      "source": [
        "First, we make the input lowercase, then we tokenize it by splitting on whitespace characters. We iterate through the list of tokens and, if the token exists in our reflections dictionary, we replace it with the value from the dictionary. So â€œIâ€ becomes â€œyouâ€, â€œyourâ€ becomes â€œmyâ€, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOGtIIORDO_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(fragment):\n",
        "    tokens = fragment.lower().split()\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token in grefs:\n",
        "            tokens[i] = grefs[token]\n",
        "    return ' '.join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjK8CQegDO_E",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can wrap everything into an interface where our Eliza will wait for the user input and generate corresponding response."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT_NwwMgDO_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def start_eliza():\n",
        "    print('Therapist\\n---------')\n",
        "    print('Talk to the program by typing in plain English, using normal upper-')\n",
        "    print('and lower-case letters and punctuation.  Enter \"quit\" when done.')\n",
        "    print('='*72)\n",
        "    print('Hello.  How are you feeling today?')\n",
        "\n",
        "    s = ''\n",
        "    while s != 'quit':\n",
        "        try:\n",
        "            s = input('> ')\n",
        "            while s[-1] in '!.':\n",
        "                s = s[:-1]\n",
        "        except:\n",
        "            s = 'quit'\n",
        "            print('Invalid Input, exit ...')\n",
        "        print(respond(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d18ONH9KDO_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_eliza()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5wSf1XADO_K",
        "colab_type": "text"
      },
      "source": [
        "Now try to add your own [RE, responses] pairs to the `gpats` to capture more questions from the user. Feel free to refer to [this](https://github.com/jezhiggins/eliza.py/blob/master/eliza.py) for more REs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZcpglIZDO_K",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/quiz.png?raw=1\" style=\"height:36pt; float:left; margin-right: 4pt\" /> <h2>Quiz</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyZ_SaRhDO_K",
        "colab_type": "text"
      },
      "source": [
        "1. For the previous text, which RE can find all words with 3, 4 and 5 characters?\n",
        "\n",
        "A. `r'.{3,5}'`\n",
        "\n",
        "B. `r'\\b.{3,5}+\\b'`\n",
        "\n",
        "C. `r'\\b\\w{3,5}\\b'`\n",
        "\n",
        "D. `r'\\w{3,5}'`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsMsvStuDO_K",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>C</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0gchtxqDO_L",
        "colab_type": "text"
      },
      "source": [
        "2. Which function can you use to find all the words that the RE in 1. matches?\n",
        "\n",
        "A. `match()`\n",
        "\n",
        "B. `search()`\n",
        "\n",
        "C. `finditer()`\n",
        "\n",
        "D. `findall()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KFBRQWBDO_N",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>C, D</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdh9FW3wDO_N",
        "colab_type": "text"
      },
      "source": [
        "3. How many words the RE in 1 mathes are there?\n",
        "\n",
        "A. 128\n",
        "\n",
        "B. 105\n",
        "\n",
        "C. 96\n",
        "\n",
        "D. 77"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITq0EUFQDO_N",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>D</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Hy8x7VDO_N",
        "colab_type": "text"
      },
      "source": [
        "4. How many words are there with 5 letters?\n",
        "\n",
        "A. 10\n",
        "\n",
        "B. 20\n",
        "\n",
        "C. 30\n",
        "\n",
        "D. 35"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2RFiWIhDO_O",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>B</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLD149YuDO_O",
        "colab_type": "text"
      },
      "source": [
        "5. What's the span of the first word with 4 letters?\n",
        "\n",
        "A. (16, 20)\n",
        "\n",
        "B. (18, 22)\n",
        "\n",
        "C. (20, 24)\n",
        "\n",
        "D. (22, 26) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeTI3TcmDO_O",
        "colab_type": "text"
      },
      "source": [
        "<hr>    <!-- please remember this! -->\n",
        "<details>\n",
        "  <summary>Click <b>here</b> to see the answer.</summary>\n",
        "  <p>C</p>\n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYFmuMglDO_P",
        "colab_type": "text"
      },
      "source": [
        "<h1 align='center' style='margin-top:2em'>\n",
        "    <img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/section_header.png?raw=1\" \n",
        "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
        "    <u>SpaCy</u>\n",
        "    <img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/section_header.png?raw=1\" \n",
        "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
        "<hr>\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rvlm2YPDO_P",
        "colab_type": "text"
      },
      "source": [
        "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "spaCy is designed specifically for production use and helps you build applications that process and â€œunderstandâ€ large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
        "\n",
        "There are many useful resources online for spaCy, among which the official [tutorial](https://spacy.io/usage/spacy-101) and the [course](https://course.spacy.io/) are extremely useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kFk9zrCDO_Q",
        "colab_type": "text"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyY3Ds7tDO_Q",
        "colab_type": "text"
      },
      "source": [
        "Follow the link [here](https://spacy.io/usage) to install spaCy with pip. Then download and install the English model by running:\n",
        "```\n",
        "python -m spacy download en_core_web_sm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5we5DlGYDO_Q",
        "colab_type": "text"
      },
      "source": [
        "spaCy is enables many different features introduced [here](https://spacy.io/usage/spacy-101#features), and we will focus on the simplest and direct features:\n",
        "- tokenization;\n",
        "- stemming; (not supported by spaCy)\n",
        "- lemmatization;\n",
        "- part-of-speech (POS) tagging;\n",
        "- sentence boundary detection and sentence segmentation;\n",
        "- dependency parsing;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2c171-mDO_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import spacy module\n",
        "import spacy\n",
        "\n",
        "# load english model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# read the text and process\n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht0xtsuBDO_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenization\n",
        "# get all separated tokens in doc\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_tG2VYbDO_V",
        "colab_type": "text"
      },
      "source": [
        "You can see from the list *tokens* that there each tokenized token is in the entry of the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7hAjUQDDO_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatization\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLnAu9kaDO_X",
        "colab_type": "text"
      },
      "source": [
        "Did you notice that almost all pronouns are subsituted with `-PRON-`?\n",
        "\n",
        "Unlike verbs and common nouns, thereâ€™s no clear base form of a personal pronoun. Should the lemma of â€œmeâ€ be â€œIâ€, or should we normalize person as well, giving â€œitâ€ â€” or maybe â€œheâ€? spaCyâ€™s solution is to introduce a novel symbol, `-PRON-`, which is used as the lemma for all personal pronouns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW6A-NbBDO_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# POS tagging\n",
        "poss = [token.pos_ for token in doc]\n",
        "print(poss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ5-vyzhDO_a",
        "colab_type": "text"
      },
      "source": [
        "Compare the outputs of the previous three lists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG-wo4ucDO_a",
        "colab_type": "text"
      },
      "source": [
        "Now let's move to sentence level features, i.e. sentence boundary detection and sentence segmentation. Unlike other libraries, spaCy uses the dependency parse to determine sentence boundaries. This is usually more accurate than a rule-based approach, but it also means youâ€™ll need a statistical model and accurate predictions.\n",
        "If your texts are closer to general-purpose news or web text, this should work well out-of-the-box. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTgob5IlDO_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = list(doc.sents)\n",
        "sent_texts = [sentence.text for sentence in sentences]\n",
        "print(sent_texts)\n",
        "print(len(sent_texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUfKkxv9DO_b",
        "colab_type": "text"
      },
      "source": [
        "However, for other texts like social media texts, your application may benefit from a custom rule-based implementation. You can either use the built-in Sentencizer or plug an entirely custom rule-based function into your processing pipeline.\n",
        "\n",
        "Let's build a rule-based sentence segmentor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERPrHVQnDO_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp_rule = English()  # just the language with no model\n",
        "sentencizer = nlp_rule.create_pipe(\"sentencizer\")\n",
        "nlp_rule.add_pipe(sentencizer)\n",
        "doc_rule = nlp_rule(text)\n",
        "sent_rule_texts = [sent.text for sent in doc_rule.sents]\n",
        "print(sent_rule_texts)\n",
        "print(len(sent_rule_texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZ_D2BkDO_d",
        "colab_type": "text"
      },
      "source": [
        "You can see clearly the difference in both methods, where the statistical dependency parse sentence segmentor generate 12 sentencs, but the rule-based sentence segmentor only yields 4 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHaz8fhoDO_e",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's have a look at the dependencies, i.e. the dependency relations between tokens, which is also used to segment sentences for the first model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ1creLSDO_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies\n",
        "deps = [token.dep_ for token in doc]\n",
        "print(deps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpWZwsT0DO_g",
        "colab_type": "text"
      },
      "source": [
        "Using spaCyâ€™s built-in [displaCy visualizer](https://spacy.io/usage/visualizers), hereâ€™s what our example sentence and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "188_UtFTDO_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(sentences, style = \"dep\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udBX7RZcDO_i",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/quiz.png?raw=1\" style=\"height:36pt; float:left; margin-right: 4pt\" /> <h2>Quiz</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdFTsOIKDO_i",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/cambridgeltl/python4cl/blob/module_2.3/resources/assessment.png?raw=1\" style=\"height:36pt; float:left; margin-right: 4pt\" /> <h2>Assessment</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDN8O5DXDO_i",
        "colab_type": "text"
      },
      "source": [
        "1. Write a function to calculate the $N_{th}$ Finbonacci number without `for` loop using the folloing formula:\n",
        "$$\n",
        "f_n = \\frac{a^n - b^n}{a - b} = \\frac{a^n - b^n}{\\sqrt{5}}\n",
        "$$\n",
        "where $a = \\frac{1 + \\sqrt{5}}{2}$ and $b = \\frac{1- \\sqrt{5}}{2}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y928jI2mDO_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "SQRT_5 = math.sqrt(5)\n",
        "def calc_fibonacci(n):\n",
        "    a = (1 + SQRT_5) / 2\n",
        "    b = (1 - SQRT_5) / 2\n",
        "    fn = int((a**n - b**n) / SQRT_5)\n",
        "    return fn\n",
        "\n",
        "print(calc_fibonacci(5))\n",
        "print(calc_fibonacci(10))\n",
        "print(calc_fibonacci(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isdQzb36DO_k",
        "colab_type": "text"
      },
      "source": [
        "2. Tokenize the following sentences with RE:\n",
        "```\n",
        "sents = \"\"\"He asked: \"When will you graduate?\"\\n\"I will get my Ph.D. degree(Doctor of Philosophy) in a few years, hehe. And get $123,45675.45.\" I answered. END\"\"\"\n",
        "```\n",
        "How many token types are there in total?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8nw6RoNDO_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "PUNCTS = ['\"', \"'\",':', ';', '?', '(',')', '[', ']', '!', '$', '&', '*', '#', '-']\n",
        "PUNCTS_REGEX =  {',':['[^ ](,)(\\s+)'], '.':['[^ ]\\.(\\s+)([^a-z])']}\n",
        "\n",
        "def my_tokenizer(sents):\n",
        "    print('Original...')\n",
        "    print(sents)\n",
        "\n",
        "    for punt in PUNCTS:\n",
        "        sents = sents.replace(punt, (' ' + punt + ' '))\n",
        "\n",
        "    for punct, regs in PUNCTS_REGEX.items():\n",
        "        for reg in regs:\n",
        "            p = re.compile(reg)\n",
        "            m = p.search(sents)\n",
        "            while m:\n",
        "                sents = sents[:m.start() + 1] + ' ' + punct + ' ' + sents[m.start() + 2:]\n",
        "                m = p.search(sents)\n",
        "    sents = re.sub(r' +', ' ', sents)        \n",
        "\n",
        "    print('After Tokenization...')\n",
        "    print(sents)\n",
        "    return sents\n",
        "\n",
        "sents = \"\"\"He asked: \"When will you graduate?\"\\n\"I will get my Ph.D. degree(Doctor of Philosophy) in a few years, hehe. And get $123,45675.45.\" I answered. END\"\"\"\n",
        "tok_sents = my_tokenizer(sents)\n",
        "print(set(tok_sents.split()))\n",
        "print(len(set(tok_sents.split())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiPgJ8g7DO_n",
        "colab_type": "text"
      },
      "source": [
        "3. Tokenize the previous sentences with SpaCy. How many token types are there in total?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIq9Ke3kDO_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import spacy module\n",
        "import spacy\n",
        "\n",
        "# load english model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# read the text and process\n",
        "doc = nlp(sents)\n",
        "print(doc)\n",
        "tokens = [token.text for token in doc]\n",
        "print(' '.join(tokens))\n",
        "print(set(tokens))\n",
        "print(len(set(tokens)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7doQ1OZjDO_v",
        "colab_type": "text"
      },
      "source": [
        "## Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1fUvegbDO_w",
        "colab_type": "text"
      },
      "source": [
        "- [How To Define Functions in Python 3](https://www.digitalocean.com/community/tutorials/how-to-define-functions-in-python-3)\n",
        "- [Regular Expression HOWTO](https://docs.python.org/3.6/howto/regex.html#regex-howto)\n",
        "- [Regular Expression Documentation](https://docs.python.org/3.6/library/re.html)\n",
        "- [Eliza](https://github.com/jezhiggins/eliza.py/blob/master/eliza.py)\n",
        "- [spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101#_title)\n",
        "- [Advanced NLP with spaCy](https://course.spacy.io/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-9H8UruDO_y",
        "colab_type": "text"
      },
      "source": [
        "# Next Module\n",
        "\n",
        "[Click here](../module_1.4/module_1.4.ipynb) to move to the next module."
      ]
    }
  ]
}